{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aaa5435a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Data\\HCMUS\\Y4\\Y4_S1\\Nhap_mon_xu_ly_ngon_ngu_tu_nhien\\Mid_term\\venv\\Lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
            "  warnings.warn(warning_message)\n"
          ]
        }
      ],
      "source": [
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "import difflib\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from paddleocr import PaddleOCR\n",
        "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16c99e07",
      "metadata": {},
      "source": [
        "## Tiền xử lý ảnh cho OCR Tesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "76b1c3df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Tìm thấy 48 file ảnh (từ 203 đến 250) cần xử lý.\n",
            "Xử lý và lưu 48 file ảnh\n",
            " Xử lý 48 file ảnh xong. Các file đã lưu vào thư mục: ../data/image\\processed_PDF1\n"
          ]
        }
      ],
      "source": [
        "INPUT_ROOT = '../data/image'\n",
        "INPUT_SUBDIR = 'PDF1'\n",
        "INPUT_DIR = os.path.join(INPUT_ROOT, INPUT_SUBDIR)\n",
        "OUTPUT_SUBDIR = f\"processed_{INPUT_SUBDIR}\"\n",
        "OUTPUT_DIR = os.path.join(INPUT_ROOT, OUTPUT_SUBDIR)\n",
        "START_PAGE = 203\n",
        "END_PAGE = 250\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# Lọc file trong phạm vi trang \n",
        "target_files = []\n",
        "try:\n",
        "    all_image_files = os.listdir(INPUT_DIR)\n",
        "    pattern = re.compile(r'PDF1-(\\d+)\\.png$', re.IGNORECASE)\n",
        "    \n",
        "    for file_name in all_image_files:\n",
        "        match = pattern.match(file_name)\n",
        "        if match:\n",
        "            page_number = int(match.group(1))\n",
        "            if START_PAGE <= page_number <= END_PAGE:\n",
        "                target_files.append(file_name)\n",
        "                \n",
        "except FileNotFoundError:\n",
        "    print(f\"Không tìm thấy thư mục đầu vào tại {INPUT_DIR}. \")\n",
        "    \n",
        "target_files.sort(key=lambda f: int(re.search(r'PDF1-(\\d+)\\.png$', f).group(1)))\n",
        "\n",
        "print(f\" Tìm thấy {len(target_files)} file ảnh (từ {START_PAGE} đến {END_PAGE}) cần xử lý.\")\n",
        "\n",
        "\n",
        "# Tiền xử lý ảnh OCR\n",
        "def preprocess_for_ocr_enforce_white_bg(image_path):\n",
        "    \"\"\"\n",
        "    Tiền xử lý nâng cao: Làm sắc nét, tăng tương phản, nhị phân hóa (Otsu), \n",
        "    khử nhiễu bằng Morphological Closing và đầu ra là NỀN TRẮNG/CHỮ ĐEN.\n",
        "    \"\"\"\n",
        "    \n",
        "    img = cv2.imread(image_path, cv2.IMREAD_COLOR) \n",
        "    if img is None:\n",
        "        print(f\"Không thể đọc ảnh tại {image_path}\")\n",
        "        return None, None\n",
        "        \n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Làm Sắc Nét (Sharpening)\n",
        "    sharpen_kernel = np.array([[ 0, -1,  0],\n",
        "                               [-1,  5, -1],\n",
        "                               [ 0, -1,  0]])\n",
        "    sharpened = cv2.filter2D(gray, -1, sharpen_kernel)\n",
        "\n",
        "    # Tăng Tương Phản (Gamma Correction)\n",
        "    gamma = 2.0 \n",
        "    inv_gamma = 1.0 / gamma\n",
        "    table = np.array([((i / 255.0) ** inv_gamma) * 255\n",
        "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "    contrasted = cv2.LUT(sharpened, table)\n",
        "    \n",
        "    # Phân Ngưỡng Nhị Phân (Tách Nền/Chữ)\n",
        "    \n",
        "    # TH1: NỀN TRẮNG / CHỮ ĐEN (THRESH_BINARY_INV)\n",
        "    _, thresh_inverted = cv2.threshold(contrasted, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    # TH2: NỀN ĐEN / CHỮ TRẮNG (THRESH_BINARY)\n",
        "    _, thresh_normal = cv2.threshold(contrasted, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    \n",
        "    # Lựa chọn ảnh có nền trắng (nền chiếm ưu thế trong văn bản)\n",
        "    # Ảnh có nền trắng sẽ có tổng số pixel trắng (255) lớn hơn đen (0) -> giá trị trung bình > 127\n",
        "    if np.mean(thresh_inverted) > np.mean(thresh_normal):\n",
        "        # thresh_inverted đã cho ra Nền Trắng / Chữ Đen (Định dạng mong muốn)\n",
        "        binary_img = thresh_inverted\n",
        "    else:\n",
        "        # thresh_normal đã cho ra Nền Trắng / Chữ Đen (Ảnh gốc bị ngược màu)\n",
        "        # Hoặc thresh_inverted cho ra Nền Đen / Chữ Trắng (Ảnh gốc xuôi màu)\n",
        "        # Đảo ngược thresh_normal để có Nền Trắng / Chữ Đen\n",
        "        binary_img = cv2.bitwise_not(thresh_normal)\n",
        "\n",
        "    # Ép buộc về NỀN TRẮNG/CHỮ ĐEN để khử nhiễu\n",
        "\n",
        "    if np.mean(binary_img) > 127:\n",
        "        # Nền Trắng/Chữ Đen -> Đảo thành Nền Đen/Chữ Trắng (Tạm thời)\n",
        "        img_for_denoise = cv2.bitwise_not(binary_img)\n",
        "    else:\n",
        "        img_for_denoise = binary_img\n",
        "        \n",
        "    # Khử Nhiễu Đốm Đen (Morphological Closing)\n",
        "    # Dùng phép Đóng (Closing) để lấp đầy các đốm đen (nhiễu) trên nền trắng\n",
        "    kernel = np.ones((2, 2), np.uint8) \n",
        "    denoised_closed = cv2.morphologyEx(img_for_denoise, cv2.MORPH_CLOSE, kernel)\n",
        "    \n",
        "    # Đảo Ngược Lại (Reverse) để có Nền Trắng/Chữ Đen\n",
        "    final_img = cv2.bitwise_not(denoised_closed)\n",
        "\n",
        "    # Kiểm tra lần cuối: Nếu vẫn là nền đen (do ảnh có quá nhiều chi tiết) -> đảo lại\n",
        "    if np.mean(final_img) < 127: \n",
        "         final_img = cv2.bitwise_not(final_img)\n",
        "         \n",
        "    return final_img, img\n",
        "\n",
        "# Xử lý và hiển thị kết quả cho ảnh mẫu\n",
        "# sample_file = 'PDF1-203.png'\n",
        "\n",
        "# if sample_file in target_files:\n",
        "#     sample_path = os.path.join(INPUT_DIR, sample_file)\n",
        "#     print(f\" Tiền xử lý ảnh mẫu để hiển thị: {sample_file}\")\n",
        "#     processed_sample, original_img = preprocess_for_ocr_enforce_white_bg(sample_path)\n",
        "    \n",
        "#     if processed_sample is not None:\n",
        "#         output_path = os.path.join(OUTPUT_DIR, f\"processed_{sample_file}\")\n",
        "#         cv2.imwrite(output_path, processed_sample)\n",
        "        \n",
        "#         original_img_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "#         plt.figure(figsize=(16, 8))\n",
        "        \n",
        "#         plt.subplot(1, 2, 1)\n",
        "#         plt.imshow(original_img_rgb)\n",
        "#         plt.title('1. Ảnh Gốc ')\n",
        "#         plt.axis('off')\n",
        "        \n",
        "#         plt.subplot(1, 2, 2)\n",
        "#         plt.imshow(processed_sample, cmap='gray') \n",
        "#         plt.title('2. Ảnh Đã Xử Lý (Nền Trắng/Chữ Đen)')\n",
        "#         plt.axis('off')\n",
        "        \n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "    \n",
        "#     print(\"-\" * 70)\n",
        "# else:\n",
        "#     print(f\" Cảnh báo: Không tìm thấy file mẫu '{sample_file}' trong thư mục đã lọc.\")\n",
        "\n",
        "print(f\"Xử lý và lưu {len(target_files)} file ảnh\")\n",
        "count = 0\n",
        "for file_name in target_files:\n",
        "    full_path = os.path.join(INPUT_DIR, file_name)\n",
        "    processed_img, _ = preprocess_for_ocr_enforce_white_bg(full_path)\n",
        "    \n",
        "    if processed_img is not None:\n",
        "        output_file_name = f\"processed_{file_name}\"\n",
        "        output_path = os.path.join(OUTPUT_DIR, output_file_name)\n",
        "        cv2.imwrite(output_path, processed_img)\n",
        "        count += 1\n",
        "\n",
        "print(f\" Xử lý {count} file ảnh xong. Các file đã lưu vào thư mục: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e03778cc",
      "metadata": {},
      "source": [
        "## OCR bằng Tesseract và Paddle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d05bc4ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# OCR Tesseract\n",
        "TESSERACT_LANGS = 'chi_sim+chi_tra+vie'\n",
        "def tesseract_ocr(img_path: str, lang: str = TESSERACT_LANGS):\n",
        "    try:\n",
        "        text_raw = pytesseract.image_to_string(\n",
        "            Image.open(img_path),\n",
        "            lang=lang,\n",
        "            config='--psm 3'\n",
        "        )\n",
        "        return [l.strip() for l in text_raw.split('\\n') if l.strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi OCR {img_path}: {e}\")\n",
        "        return []\n",
        "    \n",
        "# OCR Paddle\n",
        "ocr_paddle = PaddleOCR(\n",
        "    use_angle_cls=True,\n",
        "    lang='ch',\n",
        "    show_log=False,\n",
        "    use_gpu=True,\n",
        "    gpu_id=0,\n",
        "    gpu_mem=4096\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9d547504",
      "metadata": {},
      "outputs": [],
      "source": [
        "def strip_accents(s: str) -> str:\n",
        "    return \"\".join(\n",
        "        c for c in unicodedata.normalize(\"NFD\", s)\n",
        "        if unicodedata.category(c) != \"Mn\"\n",
        "    )\n",
        "\n",
        "def is_chinese(text: str) -> bool:\n",
        "    if not text:\n",
        "        return False\n",
        "        \n",
        "    pattern = (\n",
        "        r'^['\n",
        "        # Nhóm Hán tự\n",
        "        r'\\u4E00-\\u9FFF'  # CJK Unified Ideographs (Phổ thông)\n",
        "        r'\\u3400-\\u4DBF'  # CJK Extension A\n",
        "        r'\\uF900-\\uFAFF'  # CJK Compatibility\n",
        "        \n",
        "        # Nhóm dấu câu CJK\n",
        "        r'\\u3000-\\u303F'  # Dấu câu TQ (。 、 【 】...)\n",
        "        r'\\uFF00-\\uFFEF'  # Dấu câu Full-width (！, ？, ０-９...)\n",
        "        r'\\uFE30-\\uFE4F'  # CJK Compatibility Forms\n",
        "        \n",
        "        # Nhóm ASCII (không có kí tự Latin)\n",
        "        r'\\u0020-\\u0040'  # Khoảng trắng, Dấu câu (!..@), SỐ (0-9)\n",
        "        r'\\u005B-\\u0060'  # Dấu câu ( [ .. ` )\n",
        "        r'\\u007B-\\u007E'  # Dấu câu ( { .. ~ )\n",
        "        \n",
        "        # Nhóm Latin-1 Suplement (Giữ lại để bắt dấu » « ©)\n",
        "        # r'\\u00A0-\\u00FF'  \n",
        "        r']+$'\n",
        "    )\n",
        "    \n",
        "    return bool(re.fullmatch(pattern, text))\n",
        "\n",
        "def is_page_number_line(text: str) -> bool:\n",
        "    return re.fullmatch(r\"^\\s*\\d{1,4}\\s*$\", text) is not None\n",
        "\n",
        "def looks_like_vietnamese(text: str) -> bool:\n",
        "    norm = strip_accents(text).lower()\n",
        "    tokens = re.findall(r\"[a-z0-9]+\", norm)\n",
        "\n",
        "    def has_similar(target: str, thr: float) -> bool:\n",
        "        for t in tokens:\n",
        "            if difflib.SequenceMatcher(None, t, target).ratio() >= thr:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    return (\n",
        "        has_similar(\"buc\", 0.5) and\n",
        "        has_similar(\"thu\", 0.5) and\n",
        "        has_similar(\"viet\", 0.5) and\n",
        "        has_similar(\"cho\", 0.5) and\n",
        "        has_similar(\"chinh\", 0.5) and\n",
        "        has_similar(\"minh\", 0.5)\n",
        "    )\n",
        "\n",
        "def extract_id(text: str):\n",
        "    def norm_digits(tok: str):\n",
        "        tok = tok.strip()\n",
        "        tok = tok.replace('O', '0').replace('o', '0')\n",
        "        tok = tok.replace('I', '1').replace('l', '1').replace('|', '1').replace('!', '1')\n",
        "\n",
        "        digits = \"\".join(re.findall(r\"\\d+\", tok))  # \"50 4\" -> \"504\"\n",
        "        if not digits:\n",
        "            return None\n",
        "        if len(digits) < 3:\n",
        "            return None\n",
        "        return digits\n",
        "\n",
        "    # Chinese: 第xxx \n",
        "    m = re.search(r\"第\\s*([0-9OoIl|!\\s]{1,20})\", text)\n",
        "    if m:\n",
        "        return norm_digits(m.group(1))\n",
        "\n",
        "    # Vietnamese: so/s0/s6 ...\n",
        "    norm = strip_accents(text).lower()\n",
        "    m = re.search(r\"(?:\\bso\\b|\\bs0\\b|\\bs6\\b|s[0o6])\\s*([0-9OoIl|!\\s]{1,20})\", norm)\n",
        "    if m:\n",
        "        return norm_digits(m.group(1))\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "43e09655",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse ảnh -> Data\n",
        "DATA_MAP_TESSERACT = defaultdict(lambda: {\"src\": [], \"tgt\": [], \"vi_started\": False})\n",
        "DATA_MAP_PADDLE = defaultdict(lambda: {\"src\": [], \"tgt\": [], \"vi_started\": False})\n",
        "STATE_TESSERACT = {\"current_id\": None, \"mode\": None, \"pending_src\": []}\n",
        "STATE_PADDLE = {\"current_id\": None, \"mode\": None, \"pending_src\": []}\n",
        "\n",
        "def process_image_to_data(img_path: str, data_map, state, ocr_model):\n",
        "    if ocr_model == 'Tesseract':\n",
        "        lines = tesseract_ocr(img_path)\n",
        "    else:\n",
        "        result = ocr_paddle.ocr(img_path, cls=True)\n",
        "        if not result or result[0] is None:\n",
        "            return\n",
        "\n",
        "        blocks = result[0]\n",
        "        blocks = sorted(\n",
        "            blocks,\n",
        "            key=lambda b: (min(p[1] for p in b[0]), min(p[0] for p in b[0]))\n",
        "        )\n",
        "        lines = [b[1][0].strip() for b in blocks if b and b[1] and b[1][0]]\n",
        "\n",
        "    current_id = state[\"current_id\"]\n",
        "    mode = state[\"mode\"]\n",
        "    pending_src = state[\"pending_src\"]\n",
        "\n",
        "    for text in lines:\n",
        "        if not text:\n",
        "            continue\n",
        "        if is_page_number_line(text):\n",
        "            continue\n",
        "        \n",
        "        # print(f\"Model: {ocr_model}\\n{text}\")\n",
        "\n",
        "        # Title tiếng Việt -> mode=vi (lấy ID) và chỉ xử lý như title nếu extract được ID\n",
        "        if looks_like_vietnamese(text):\n",
        "            maybe_id = extract_id(text)\n",
        "            if maybe_id:\n",
        "                # Title thật\n",
        "                current_id = maybe_id\n",
        "                _ = data_map[current_id]\n",
        "                if pending_src:\n",
        "                    data_map[current_id][\"src\"].extend(pending_src)\n",
        "                    pending_src.clear()\n",
        "\n",
        "                mode = \"vi\"\n",
        "                if not data_map[current_id][\"vi_started\"]:\n",
        "                    data_map[current_id][\"tgt\"] = []\n",
        "                    data_map[current_id][\"vi_started\"] = True\n",
        "                \n",
        "                data_map[current_id][\"tgt\"].append(\n",
        "                    f\"Bức thư viết cho chính mình số {current_id}\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "        # Anchor ID (ưu tiên từ tiếng Trung: 第...)\n",
        "        found_id = extract_id(text)\n",
        "        if found_id:\n",
        "            current_id = found_id\n",
        "            _ = data_map[current_id]\n",
        "            if pending_src:\n",
        "                data_map[current_id][\"src\"].extend(pending_src)\n",
        "                pending_src.clear()\n",
        "\n",
        "            mode = \"zh\" if is_chinese(text) else \"vi\"\n",
        "            if mode == \"vi\" and (not data_map[current_id][\"vi_started\"]):\n",
        "                data_map[current_id][\"tgt\"] = []\n",
        "                data_map[current_id][\"vi_started\"] = True\n",
        "            \n",
        "            data_map[current_id][\"src\"].append(\n",
        "                f\"写 给 自己 的 第 {current_id} 封 信\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Chưa có ID -> giữ tiếng Trung để chờ, bỏ qua Pinyin\n",
        "        if current_id is None:\n",
        "            if is_chinese(text):\n",
        "                pending_src.append(text)\n",
        "            continue\n",
        "\n",
        "        # Gom nội dung theo mode\n",
        "        if is_chinese(text):\n",
        "            if mode == \"vi\":\n",
        "                # Gặp tiếng Trung sau tiếng Việt -> Reset để chờ ID mới\n",
        "                pending_src.append(text)\n",
        "                current_id = None\n",
        "                mode = \"zh\"\n",
        "            else:\n",
        "                data_map[current_id][\"src\"].append(text)\n",
        "        else:\n",
        "            if mode == \"vi\":\n",
        "                data_map[current_id][\"tgt\"].append(text)\n",
        "\n",
        "    state[\"current_id\"] = current_id\n",
        "    state[\"mode\"] = mode\n",
        "    state[\"pending_src\"] = pending_src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c714e56d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang xử lý: ../data/image\\PDF1/PDF1-203.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-204.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-205.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-206.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-207.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-208.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-209.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-210.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-211.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-212.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-213.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-214.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-215.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-216.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-217.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-218.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-219.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-220.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-221.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-222.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-223.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-224.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-225.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-226.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-227.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-228.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-229.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-230.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-231.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-232.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-233.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-234.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-235.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-236.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-237.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-238.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-239.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-240.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-241.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-242.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-243.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-244.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-245.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-246.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-247.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-248.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-249.png...\n",
            "Đang xử lý: ../data/image\\PDF1/PDF1-250.png...\n"
          ]
        }
      ],
      "source": [
        "# Main process\n",
        "IMG_DIR = \"../data/image\"\n",
        "OUT_DIR = \"../output/ocr\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "for p in range(START_PAGE, END_PAGE + 1):\n",
        "    img_path_tesseract = os.path.join(IMG_DIR, f\"processed_{INPUT_SUBDIR}/processed_PDF1-{p}.png\")\n",
        "    img_path_paddle = os.path.join(IMG_DIR, f\"{INPUT_SUBDIR}/PDF1-{p}.png\")\n",
        "    if os.path.exists(img_path_paddle):\n",
        "        print(f\"Đang xử lý: {img_path_paddle}...\")\n",
        "        process_image_to_data(img_path_tesseract, DATA_MAP_TESSERACT, STATE_TESSERACT, 'Tesseract')\n",
        "        process_image_to_data(img_path_paddle, DATA_MAP_PADDLE, STATE_PADDLE, 'Paddle')\n",
        "\n",
        "rows = []\n",
        "all_ids = set(DATA_MAP_PADDLE.keys()) | set(DATA_MAP_TESSERACT.keys())\n",
        "\n",
        "def sort_key(x):\n",
        "    try:\n",
        "        return int(str(x))\n",
        "    except:\n",
        "        return str(x)\n",
        "\n",
        "for sid in sorted(all_ids, key=sort_key):\n",
        "    # Lấy Tiếng Trung (src) từ PADDLE Map\n",
        "    paddle_data = DATA_MAP_PADDLE.get(sid, {\"src\": [], \"tgt\": []})\n",
        "    src_text = \" \".join(paddle_data[\"src\"]).strip()\n",
        "    \n",
        "    # Lấy Tiếng Việt (tgt) từ TESSERACT Map\n",
        "    tesseract_data = DATA_MAP_TESSERACT.get(sid, {\"src\": [], \"tgt\": []})\n",
        "    tgt_text = \" \".join(tesseract_data[\"tgt\"]).strip()\n",
        "    # print(f\"{sid}\\n\")\n",
        "    # print(f\"{src_text}\\n\")\n",
        "    # print(f\"{tgt_text}\\n\")\n",
        "    if src_text or tgt_text:\n",
        "        rows.append({\n",
        "            \"src_id\": sid,\n",
        "            \"src_lang\": src_text,  # src: Paddle\n",
        "            \"tgt_lang\": tgt_text   # tgt: Tesseract\n",
        "        })\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Lọc ID \n",
        "ID_START = 501\n",
        "ID_END = 625\n",
        "df = df[df['src_id'].str.isdigit()].copy()\n",
        "df['src_id_int'] = df['src_id'].astype(int)\n",
        "df = df[(df['src_id_int'] >= ID_START) & (df['src_id_int'] <= ID_END)]\n",
        "df = df.sort_values('src_id_int').drop(columns=['src_id_int']).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87fddcaf",
      "metadata": {},
      "source": [
        "## Hậu xử lý (post-processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ecc47431",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Định nghĩa bộ lọc Regex\n",
        "def is_vietnamese(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return False\n",
        "    \n",
        "    # \\u0020-\\u007E: Bao gồm a-z, A-Z, 0-9, dấu câu chuẩn (!, @, #, ., ?, ...)\n",
        "    # Phần còn lại: Các nguyên âm có dấu và chữ Đ/đ của tiếng Việt\n",
        "    vietnamese_chars = (\n",
        "        \"àáạảãâầấậẩẫăằắặẳẵ\"\n",
        "        \"èéẹẻẽêềếệểễ\"\n",
        "        \"ìíịỉĩ\"\n",
        "        \"òóọỏõôồốộổỗơờớợởỡ\"\n",
        "        \"ùúụủũưừứựửữ\"\n",
        "        \"ỳýỵỷỹ\"\n",
        "        \"đ\"\n",
        "        \"ÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴ\"\n",
        "        \"ÈÉẸẺẼÊỀẾỆỂỄ\"\n",
        "        \"ÌÍỊỈĨ\"\n",
        "        \"ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ\"\n",
        "        \"ÙÚỤỦŨƯỪỨỰỬỮ\"\n",
        "        \"ỲÝỴỶỸ\"\n",
        "        \"Đ\"\n",
        "    )\n",
        "    \n",
        "    # Logic: Chuỗi hợp lệ chỉ được chứa các ký tự nằm trong 2 nhóm này\n",
        "    pattern = f'^[\\u0020-\\u007E{vietnamese_chars}]+$'\n",
        "    \n",
        "    return bool(re.match(pattern, text))\n",
        "\n",
        "# Drop các dòng bị thiếu (NaN) hoặc rỗng ở 1 trong 2 cột\n",
        "# Chuyển chuỗi chỉ có khoảng trắng thành NaN \n",
        "df = df.replace(r'^\\s*$', float('nan'), regex=True)\n",
        "df.dropna(subset=['src_lang', 'tgt_lang'], inplace=True)\n",
        "\n",
        "# Logic: Giữ lại dòng khi (Check Trung == True) và (Check Việt == True)\n",
        "# Các dòng False sẽ bị loại bỏ\n",
        "df_clean = df[\n",
        "    df['src_lang'].apply(is_chinese) & \n",
        "    df['tgt_lang'].apply(is_vietnamese)\n",
        "].copy()\n",
        "df_clean.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "46619592",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: ../output/ocr\\PDF1_501_625.csv\n"
          ]
        }
      ],
      "source": [
        "# Xuất CSV\n",
        "if not df.empty:\n",
        "    min_id = df_clean['src_id'].astype(int).min()\n",
        "    max_id = df_clean['src_id'].astype(int).max()\n",
        "else:\n",
        "    min_id = max_id = 0\n",
        "\n",
        "out_csv = os.path.join(OUT_DIR, f\"PDF1_{min_id}_{max_id}.csv\")\n",
        "df_clean.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"Saved:\", out_csv)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
