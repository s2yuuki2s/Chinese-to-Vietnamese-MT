{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d449b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import thư viện cần thiết\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from paddleocr import PaddleOCR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54cf5f",
   "metadata": {},
   "source": [
    "1. Chuyển file PDF sang các file ảnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58d089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xử lí PDF 1 thành hình ảnh dạng png từ trang 203 đến trang 395 (theo id)\n",
    "pdf_file = \"../data/pdf/PDF1.pdf\"\n",
    "output_folder = \"../image/PDF1\"\n",
    "\n",
    "\n",
    "images = convert_from_path(\n",
    "    pdf_file,\n",
    "    first_page=203,\n",
    "    last_page=395\n",
    ")\n",
    "\n",
    "for page_num, img in enumerate(images, start=203):\n",
    "    filename = f\"PDF1-{page_num}.png\"\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    img.save(save_path, \"PNG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d829e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xử lí PDF 3 thành hình ảnh dạng png từ trang 12 đến trang 118 (theo id)\n",
    "pdf_file = \"../data/pdf/PDF3.pdf\"\n",
    "output_folder = \"../image/PDF3\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "start_page = 12\n",
    "end_page = 118\n",
    "\n",
    "images = convert_from_path(pdf_file, first_page=start_page, last_page=end_page)\n",
    "\n",
    "for page_num, img in zip(range(start_page, end_page + 1), images):\n",
    "    page_str = f\"{page_num:03d}\"           \n",
    "    filename = f\"PDF3-{page_str}.png\"\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    img.save(save_path, \"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f6eff",
   "metadata": {},
   "source": [
    "2. Tiền xử lí ảnh trước khi OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6752acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Đã xử lý 10/47 ảnh\n",
      " Đã xử lý 20/47 ảnh\n",
      " Đã xử lý 30/47 ảnh\n",
      " Đã xử lý 40/47 ảnh\n",
      "\n",
      "  Đã lưu 47 ảnh vào: ../image\\PDF1_processed\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "INPUT_ROOT = '../image'\n",
    "INPUT_SUBDIR = 'PDF1'\n",
    "INPUT_DIR = os.path.join(INPUT_ROOT, INPUT_SUBDIR)\n",
    "\n",
    "OUTPUT_SUBDIR = 'PDF1_processed'\n",
    "OUTPUT_DIR = os.path.join(INPUT_ROOT, OUTPUT_SUBDIR)\n",
    "\n",
    "START_PAGE = 250\n",
    "END_PAGE = 296\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"✅ Đã tạo thư mục đầu ra: {OUTPUT_DIR}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. LỌC FILE THEO SỐ TRANG (GIỮ NGUYÊN)\n",
    "# =====================================================\n",
    "target_files = []\n",
    "pattern = re.compile(r'PDF1-(\\d+)\\.png$', re.IGNORECASE)\n",
    "\n",
    "try:\n",
    "    all_files = os.listdir(INPUT_DIR)\n",
    "    for file_name in all_files:\n",
    "        match = pattern.match(file_name)\n",
    "        if match:\n",
    "            page_number = int(match.group(1))\n",
    "            if START_PAGE <= page_number <= END_PAGE:\n",
    "                target_files.append(file_name)\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Không tìm thấy thư mục đầu vào: {INPUT_DIR}\")\n",
    "\n",
    "target_files.sort(key=lambda f: int(pattern.search(f).group(1)))\n",
    "\n",
    "# =====================================================\n",
    "# 3. HÀM XỬ LÝ TỐI ƯU OCR: CHỐNG MẤT NÉT & LÀM ĐẬM CHỮ\n",
    "# =====================================================\n",
    "def process_optimized_for_ocr(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None, None\n",
    "\n",
    "    # 1. Chuyển sang Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. Tăng cường độ tương phản cục bộ (CLAHE)\n",
    "    # Giúp các dòng chữ nhạt màu như \"số 641\" hiện rõ hơn khỏi nền\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "\n",
    "    # 3. Adaptive Thresholding (Phân ngưỡng thích nghi)\n",
    "    # Thay vì dùng ngưỡng cố định cho cả ảnh, nó tính toán từng vùng nhỏ\n",
    "    # giúp giữ lại nét chữ mảnh cực tốt.\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        enhanced, 255, \n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 21, 15\n",
    "    )\n",
    "\n",
    "    # 4. KHỬ NHIỄU NHẸ (Dùng Median Blur kích thước nhỏ nhất)\n",
    "    # Để xóa các đốm đen li ti mà không làm hỏng cấu trúc chữ\n",
    "    denoised = cv2.medianBlur(binary, 3)\n",
    "\n",
    "    # 5. LÀM ĐẬM NÉT CHỮ (Erosion)\n",
    "    # Với ảnh nền trắng chữ đen, phép Erode sẽ làm các vùng đen (chữ) dày lên.\n",
    "    # Điều này cực kỳ quan trọng để Tesseract nhận diện được các chữ bị nhạt/đứt nét.\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    final_img = cv2.erode(denoised, kernel, iterations=1)\n",
    "\n",
    "    return final_img, img\n",
    "\n",
    "# =====================================================\n",
    "# 4. THỰC THI XỬ LÝ ĐỒNG LOẠT\n",
    "# =====================================================\n",
    "\n",
    "count = 0\n",
    "for file_name in target_files:\n",
    "    full_path = os.path.join(INPUT_DIR, file_name)\n",
    "    processed_img, _ = process_optimized_for_ocr(full_path)\n",
    "\n",
    "    if processed_img is not None:\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"processed_{file_name}\")\n",
    "        cv2.imwrite(output_path, processed_img)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\" Đã xử lý {count}/{len(target_files)} ảnh\")\n",
    "\n",
    "print(f\"\\n  Đã lưu {count} ảnh vào: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6740e",
   "metadata": {},
   "source": [
    "3. Xử lí các ảnh thông qua sử dụng Paddle OCR và Teserract OCR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99267634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR Tesseract\n",
    "TESSERACT_LANGS = 'chi_sim+chi_tra+vie'\n",
    "def tesseract_ocr(img_path: str, lang: str = TESSERACT_LANGS):\n",
    "    try:\n",
    "        text_raw = pytesseract.image_to_string(\n",
    "            Image.open(img_path),\n",
    "            lang=lang,\n",
    "            config='--psm 3'\n",
    "        )\n",
    "        return [l.strip() for l in text_raw.split('\\n') if l.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi OCR {img_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "# OCR Paddle\n",
    "ocr_paddle = PaddleOCR(\n",
    "    use_angle_cls=True,\n",
    "    lang='ch',\n",
    "    show_log=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a86760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def is_chinese(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "        \n",
    "    pattern = (\n",
    "        r'^['\n",
    "        # Nhóm Hán tự\n",
    "        r'\\u4E00-\\u9FFF'  # CJK Unified Ideographs (Phổ thông)\n",
    "        r'\\u3400-\\u4DBF'  # CJK Extension A\n",
    "        r'\\uF900-\\uFAFF'  # CJK Compatibility\n",
    "        \n",
    "        # Nhóm dấu câu CJK\n",
    "        r'\\u3000-\\u303F'  # Dấu câu TQ (。 、 【 】...)\n",
    "        r'\\uFF00-\\uFFEF'  # Dấu câu Full-width (！, ？, ０-９...)\n",
    "        r'\\uFE30-\\uFE4F'  # CJK Compatibility Forms\n",
    "        \n",
    "        # Nhóm ASCII (không có kí tự Latin)\n",
    "        r'\\u0020-\\u0040'  # Khoảng trắng, Dấu câu (!..@), SỐ (0-9)\n",
    "        r'\\u005B-\\u0060'  # Dấu câu ( [ .. ` )\n",
    "        r'\\u007B-\\u007E'  # Dấu câu ( { .. ~ )\n",
    "        \n",
    "        # Nhóm Latin-1 Suplement (Giữ lại để bắt dấu » « ©)\n",
    "        # r'\\u00A0-\\u00FF'  \n",
    "        r']+$'\n",
    "    )\n",
    "    \n",
    "    return bool(re.fullmatch(pattern, text))\n",
    "\n",
    "def is_page_number_line(text: str) -> bool:\n",
    "    return re.fullmatch(r\"^\\s*\\d{1,4}\\s*$\", text) is not None\n",
    "\n",
    "def looks_like_vietnamese(text: str) -> bool:\n",
    "    norm = strip_accents(text).lower()\n",
    "    tokens = re.findall(r\"[a-z0-9]+\", norm)\n",
    "\n",
    "    def has_similar(target: str, thr: float) -> bool:\n",
    "        for t in tokens:\n",
    "            if difflib.SequenceMatcher(None, t, target).ratio() >= thr:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    return (\n",
    "        has_similar(\"buc\", 0.5) and\n",
    "        has_similar(\"thu\", 0.5) and\n",
    "        has_similar(\"viet\", 0.5) and\n",
    "        has_similar(\"cho\", 0.5) and\n",
    "        has_similar(\"chinh\", 0.5) and\n",
    "        has_similar(\"minh\", 0.5)\n",
    "    )\n",
    "\n",
    "def extract_id(text: str):\n",
    "    def norm_digits(tok: str):\n",
    "        tok = tok.strip()\n",
    "        tok = tok.replace('O', '0').replace('o', '0')\n",
    "        tok = tok.replace('I', '1').replace('l', '1').replace('|', '1').replace('!', '1')\n",
    "\n",
    "        digits = \"\".join(re.findall(r\"\\d+\", tok))  # \"50 4\" -> \"504\"\n",
    "        if not digits:\n",
    "            return None\n",
    "        if len(digits) < 3:\n",
    "            return None\n",
    "        return digits\n",
    "\n",
    "    # Chinese: 第xxx \n",
    "    m = re.search(r\"第\\s*([0-9OoIl|!\\s]{1,20})\", text)\n",
    "    if m:\n",
    "        return norm_digits(m.group(1))\n",
    "\n",
    "    # Vietnamese: so/s0/s6 ...\n",
    "    norm = strip_accents(text).lower()\n",
    "    m = re.search(r\"(?:\\bso\\b|\\bs0\\b|\\bs6\\b|s[0o6])\\s*([0-9OoIl|!\\s]{1,20})\", norm)\n",
    "    if m:\n",
    "        return norm_digits(m.group(1))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8828b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse ảnh -> Data\n",
    "DATA_MAP_TESSERACT = defaultdict(lambda: {\"src\": [], \"tgt\": [], \"vi_started\": False})\n",
    "DATA_MAP_PADDLE = defaultdict(lambda: {\"src\": [], \"tgt\": [], \"vi_started\": False})\n",
    "STATE_TESSERACT = {\"current_id\": None, \"mode\": None, \"pending_src\": []}\n",
    "STATE_PADDLE = {\"current_id\": None, \"mode\": None, \"pending_src\": []}\n",
    "\n",
    "def process_image_to_data(img_path: str, data_map, state, ocr_model):\n",
    "    if ocr_model == 'Tesseract':\n",
    "        lines = tesseract_ocr(img_path)\n",
    "    else:\n",
    "        result = ocr_paddle.ocr(img_path, cls=True)\n",
    "        if not result or result[0] is None:\n",
    "            return\n",
    "\n",
    "        blocks = result[0]\n",
    "        blocks = sorted(\n",
    "            blocks,\n",
    "            key=lambda b: (min(p[1] for p in b[0]), min(p[0] for p in b[0]))\n",
    "        )\n",
    "        lines = [b[1][0].strip() for b in blocks if b and b[1] and b[1][0]]\n",
    "\n",
    "    current_id = state[\"current_id\"]\n",
    "    mode = state[\"mode\"]\n",
    "    pending_src = state[\"pending_src\"]\n",
    "\n",
    "    for text in lines:\n",
    "        if not text:\n",
    "            continue\n",
    "        if is_page_number_line(text):\n",
    "            continue\n",
    "        \n",
    "        # print(f\"Model: {ocr_model}\\n{text}\")\n",
    "\n",
    "        # Title tiếng Việt -> mode=vi (lấy ID) và chỉ xử lý như title nếu extract được ID\n",
    "        if looks_like_vietnamese(text):\n",
    "            maybe_id = extract_id(text)\n",
    "            if maybe_id:\n",
    "                # Title thật\n",
    "                current_id = maybe_id\n",
    "                _ = data_map[current_id]\n",
    "                if pending_src:\n",
    "                    data_map[current_id][\"src\"].extend(pending_src)\n",
    "                    pending_src.clear()\n",
    "\n",
    "                mode = \"vi\"\n",
    "                if not data_map[current_id][\"vi_started\"]:\n",
    "                    data_map[current_id][\"tgt\"] = []\n",
    "                    data_map[current_id][\"vi_started\"] = True\n",
    "                \n",
    "                data_map[current_id][\"tgt\"].append(\n",
    "                    f\"Bức thư viết cho chính mình số {current_id}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        # Anchor ID (ưu tiên từ tiếng Trung: 第...)\n",
    "        found_id = extract_id(text)\n",
    "        if found_id:\n",
    "            current_id = found_id\n",
    "            _ = data_map[current_id]\n",
    "            if pending_src:\n",
    "                data_map[current_id][\"src\"].extend(pending_src)\n",
    "                pending_src.clear()\n",
    "\n",
    "            mode = \"zh\" if is_chinese(text) else \"vi\"\n",
    "            if mode == \"vi\" and (not data_map[current_id][\"vi_started\"]):\n",
    "                data_map[current_id][\"tgt\"] = []\n",
    "                data_map[current_id][\"vi_started\"] = True\n",
    "            \n",
    "            data_map[current_id][\"src\"].append(\n",
    "                f\"写 给 自己 的 第 {current_id} 封 信\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Chưa có ID -> giữ tiếng Trung để chờ, bỏ qua Pinyin\n",
    "        if current_id is None:\n",
    "            if is_chinese(text):\n",
    "                pending_src.append(text)\n",
    "            continue\n",
    "\n",
    "        # Gom nội dung theo mode\n",
    "        if is_chinese(text):\n",
    "            if mode == \"vi\":\n",
    "                # Gặp tiếng Trung sau tiếng Việt -> Reset để chờ ID mới\n",
    "                pending_src.append(text)\n",
    "                current_id = None\n",
    "                mode = \"zh\"\n",
    "            else:\n",
    "                data_map[current_id][\"src\"].append(text)\n",
    "        else:\n",
    "            if mode == \"vi\":\n",
    "                data_map[current_id][\"tgt\"].append(text)\n",
    "\n",
    "    state[\"current_id\"] = current_id\n",
    "    state[\"mode\"] = mode\n",
    "    state[\"pending_src\"] = pending_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7cbcb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý: ../image\\PDF1/PDF1-250.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-251.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-252.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-253.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-254.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-255.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-256.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-257.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-258.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-259.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-260.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-261.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-262.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-263.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-264.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-265.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-266.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-267.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-268.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-269.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-270.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-271.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-272.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-273.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-274.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-275.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-276.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-277.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-278.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-279.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-280.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-281.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-282.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-283.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-284.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-285.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-286.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-287.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-288.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-289.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-290.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-291.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-292.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-293.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-294.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-295.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-296.png...\n"
     ]
    }
   ],
   "source": [
    "# Main process\n",
    "IMG_DIR = \"../image\"\n",
    "OUT_DIR = \"../OCR\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "for p in range(START_PAGE, END_PAGE + 1):\n",
    "    img_path_tesseract = os.path.join(IMG_DIR, f\"{INPUT_SUBDIR}_processed/processed_PDF1-{p}.png\")\n",
    "    img_path_paddle = os.path.join(IMG_DIR, f\"{INPUT_SUBDIR}/PDF1-{p}.png\")\n",
    "    if os.path.exists(img_path_paddle):\n",
    "        print(f\"Đang xử lý: {img_path_paddle}...\")\n",
    "        process_image_to_data(img_path_tesseract, DATA_MAP_TESSERACT, STATE_TESSERACT, 'Tesseract')\n",
    "        process_image_to_data(img_path_paddle, DATA_MAP_PADDLE, STATE_PADDLE, 'Paddle')\n",
    "\n",
    "rows = []\n",
    "all_ids = set(DATA_MAP_PADDLE.keys()) | set(DATA_MAP_TESSERACT.keys())\n",
    "\n",
    "def sort_key(x):\n",
    "    try:\n",
    "        return int(str(x))\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "for sid in sorted(all_ids, key=sort_key):\n",
    "    # Lấy Tiếng Trung (src) từ PADDLE Map\n",
    "    paddle_data = DATA_MAP_PADDLE.get(sid, {\"src\": [], \"tgt\": []})\n",
    "    src_text = \" \".join(paddle_data[\"src\"]).strip()\n",
    "    \n",
    "    # Lấy Tiếng Việt (tgt) từ TESSERACT Map\n",
    "    tesseract_data = DATA_MAP_TESSERACT.get(sid, {\"src\": [], \"tgt\": []})\n",
    "    tgt_text = \" \".join(tesseract_data[\"tgt\"]).strip()\n",
    "    # print(f\"{sid}\\n\")\n",
    "    # print(f\"{src_text}\\n\")\n",
    "    # print(f\"{tgt_text}\\n\")\n",
    "    if src_text or tgt_text:\n",
    "        rows.append({\n",
    "            \"src_id\": sid,\n",
    "            \"src_lang\": src_text,  # src: Paddle\n",
    "            \"tgt_lang\": tgt_text   # tgt: Tesseract\n",
    "        })\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Lọc ID \n",
    "ID_START = 626\n",
    "ID_END = 750\n",
    "df = df[df['src_id'].str.isdigit()].copy()\n",
    "df['src_id_int'] = df['src_id'].astype(int)\n",
    "df = df[(df['src_id_int'] >= ID_START) & (df['src_id_int'] <= ID_END)]\n",
    "df = df.sort_values('src_id_int').drop(columns=['src_id_int']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c5aae",
   "metadata": {},
   "source": [
    "4. Hậu xử lí OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d0ecf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../OCR\\PDF1_626_750.csv\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa bộ lọc Regex\n",
    "def is_vietnamese(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return False\n",
    "    \n",
    "    # \\u0020-\\u007E: Bao gồm a-z, A-Z, 0-9, dấu câu chuẩn (!, @, #, ., ?, ...)\n",
    "    # Phần còn lại: Các nguyên âm có dấu và chữ Đ/đ của tiếng Việt\n",
    "    vietnamese_chars = (\n",
    "        \"àáạảãâầấậẩẫăằắặẳẵ\"\n",
    "        \"èéẹẻẽêềếệểễ\"\n",
    "        \"ìíịỉĩ\"\n",
    "        \"òóọỏõôồốộổỗơờớợởỡ\"\n",
    "        \"ùúụủũưừứựửữ\"\n",
    "        \"ỳýỵỷỹ\"\n",
    "        \"đ\"\n",
    "        \"ÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴ\"\n",
    "        \"ÈÉẸẺẼÊỀẾỆỂỄ\"\n",
    "        \"ÌÍỊỈĨ\"\n",
    "        \"ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ\"\n",
    "        \"ÙÚỤỦŨƯỪỨỰỬỮ\"\n",
    "        \"ỲÝỴỶỸ\"\n",
    "        \"Đ\"\n",
    "    )\n",
    "    \n",
    "    # Logic: Chuỗi hợp lệ chỉ được chứa các ký tự nằm trong 2 nhóm này\n",
    "    # LƯU Ý: Nếu văn bản của bạn có dấu nháy Unicode (“ ”), bạn phải thêm chúng vào pattern này\n",
    "    # hoặc xử lý chúng trước khi lọc, nếu không dòng đó sẽ bị False.\n",
    "    pattern = f'^[\\u0020-\\u007E{vietnamese_chars}]+$'\n",
    "    \n",
    "    return bool(re.match(pattern, text))\n",
    "\n",
    "# --- BỔ SUNG LOGIC XÓA TRƯỚC CHỮ 写 ---\n",
    "def trim_before_xie(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    idx = text.find('写')\n",
    "    if idx != -1:\n",
    "        return text[idx:]\n",
    "    return text\n",
    "\n",
    "# Áp dụng xóa ký tự trước chữ '写' cho cột src_lang\n",
    "df['src_lang'] = df['src_lang'].apply(trim_before_xie)\n",
    "\n",
    "# ---------------------------------------\n",
    "# ĐOẠN CODE GỐC CỦA BẠN TIẾP TỤC TẠI ĐÂY\n",
    "# ---------------------------------------\n",
    "\n",
    "# Drop các dòng bị thiếu (NaN) hoặc rỗng ở 1 trong 2 cột\n",
    "# Chuyển chuỗi chỉ có khoảng trắng thành NaN \n",
    "df = df.replace(r'^\\s*$', float('nan'), regex=True)\n",
    "df.dropna(subset=['src_lang', 'tgt_lang'], inplace=True)\n",
    "\n",
    "# Logic: Giữ lại dòng khi (Check Trung == True) và (Check Việt == True)\n",
    "# Các dòng False sẽ bị loại bỏ\n",
    "df_clean = df[\n",
    "    df['src_lang'].apply(is_chinese) & \n",
    "    df['tgt_lang'].apply(is_vietnamese)\n",
    "].copy()\n",
    "df_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Xuất CSV\n",
    "if not df_clean.empty:\n",
    "    min_id = df_clean['src_id'].astype(int).min()\n",
    "    max_id = df_clean['src_id'].astype(int).max()\n",
    "else:\n",
    "    min_id = max_id = 0\n",
    "\n",
    "out_csv = os.path.join(OUT_DIR, f\"PDF1_{min_id}_{max_id}.csv\")\n",
    "df_clean.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved:\", out_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
