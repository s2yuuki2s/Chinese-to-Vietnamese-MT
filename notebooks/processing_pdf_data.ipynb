{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d449b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from paddleocr import PaddleOCR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54cf5f",
   "metadata": {},
   "source": [
    "1. Chuy·ªÉn file PDF sang c√°c file ·∫£nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58d089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X·ª≠ l√≠ PDF 1 th√†nh h√¨nh ·∫£nh d·∫°ng png t·ª´ trang 203 ƒë·∫øn trang 395 (theo id)\n",
    "pdf_file = \"../data/pdf/PDF1.pdf\"\n",
    "output_folder = \"../image/PDF1\"\n",
    "\n",
    "\n",
    "images = convert_from_path(\n",
    "    pdf_file,\n",
    "    first_page=203,\n",
    "    last_page=395\n",
    ")\n",
    "\n",
    "for page_num, img in enumerate(images, start=203):\n",
    "    filename = f\"PDF1-{page_num}.png\"\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    img.save(save_path, \"PNG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d829e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X·ª≠ l√≠ PDF 3 th√†nh h√¨nh ·∫£nh d·∫°ng png t·ª´ trang 12 ƒë·∫øn trang 118 (theo id)\n",
    "pdf_file = \"../data/pdf/PDF3.pdf\"\n",
    "output_folder = \"../image/PDF3\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "start_page = 12\n",
    "end_page = 118\n",
    "\n",
    "images = convert_from_path(pdf_file, first_page=start_page, last_page=end_page)\n",
    "\n",
    "for page_num, img in zip(range(start_page, end_page + 1), images):\n",
    "    page_str = f\"{page_num:03d}\"           \n",
    "    filename = f\"PDF3-{page_str}.png\"\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    img.save(save_path, \"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f6eff",
   "metadata": {},
   "source": [
    "2. Ti·ªÅn x·ª≠ l√≠ ·∫£nh tr∆∞·ªõc khi OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6752acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ T√¨m th·∫•y 47 ·∫£nh c·∫ßn x·ª≠ l√Ω t·ª´ trang 250 ƒë·∫øn 296\n",
      "üöÄ ƒêang x·ª≠ l√Ω ƒë·ªìng lo·∫°t...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω 10/47 ·∫£nh...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω 20/47 ·∫£nh...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω 30/47 ·∫£nh...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω 40/47 ·∫£nh...\n",
      "\n",
      "‚ú® Ho√†n t·∫•t! ƒê√£ l∆∞u 47 ·∫£nh v√†o: ../image\\PDF1_processed\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "INPUT_ROOT = '../image'\n",
    "INPUT_SUBDIR = 'PDF1'\n",
    "INPUT_DIR = os.path.join(INPUT_ROOT, INPUT_SUBDIR)\n",
    "\n",
    "OUTPUT_SUBDIR = 'PDF1_processed'\n",
    "OUTPUT_DIR = os.path.join(INPUT_ROOT, OUTPUT_SUBDIR)\n",
    "\n",
    "START_PAGE = 250\n",
    "END_PAGE = 296\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c ƒë·∫ßu ra: {OUTPUT_DIR}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. L·ªåC FILE THEO S·ªê TRANG (GI·ªÆ NGUY√äN)\n",
    "# =====================================================\n",
    "target_files = []\n",
    "pattern = re.compile(r'PDF1-(\\d+)\\.png$', re.IGNORECASE)\n",
    "\n",
    "try:\n",
    "    all_files = os.listdir(INPUT_DIR)\n",
    "    for file_name in all_files:\n",
    "        match = pattern.match(file_name)\n",
    "        if match:\n",
    "            page_number = int(match.group(1))\n",
    "            if START_PAGE <= page_number <= END_PAGE:\n",
    "                target_files.append(file_name)\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c ƒë·∫ßu v√†o: {INPUT_DIR}\")\n",
    "\n",
    "target_files.sort(key=lambda f: int(pattern.search(f).group(1)))\n",
    "print(f\"üìÑ T√¨m th·∫•y {len(target_files)} ·∫£nh c·∫ßn x·ª≠ l√Ω t·ª´ trang {START_PAGE} ƒë·∫øn {END_PAGE}\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. H√ÄM X·ª¨ L√ù T·ªêI ∆ØU OCR: CH·ªêNG M·∫§T N√âT & L√ÄM ƒê·∫¨M CH·ªÆ\n",
    "# =====================================================\n",
    "def process_optimized_for_ocr(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None, None\n",
    "\n",
    "    # 1. Chuy·ªÉn sang Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. TƒÉng c∆∞·ªùng ƒë·ªô t∆∞∆°ng ph·∫£n c·ª•c b·ªô (CLAHE)\n",
    "    # Gi√∫p c√°c d√≤ng ch·ªØ nh·∫°t m√†u nh∆∞ \"s·ªë 641\" hi·ªán r√µ h∆°n kh·ªèi n·ªÅn\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "\n",
    "    # 3. Adaptive Thresholding (Ph√¢n ng∆∞·ª°ng th√≠ch nghi)\n",
    "    # Thay v√¨ d√πng ng∆∞·ª°ng c·ªë ƒë·ªãnh cho c·∫£ ·∫£nh, n√≥ t√≠nh to√°n t·ª´ng v√πng nh·ªè\n",
    "    # gi√∫p gi·ªØ l·∫°i n√©t ch·ªØ m·∫£nh c·ª±c t·ªët.\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        enhanced, 255, \n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 21, 15\n",
    "    )\n",
    "\n",
    "    # 4. KH·ª¨ NHI·ªÑU NH·∫∏ (D√πng Median Blur k√≠ch th∆∞·ªõc nh·ªè nh·∫•t)\n",
    "    # ƒê·ªÉ x√≥a c√°c ƒë·ªëm ƒëen li ti m√† kh√¥ng l√†m h·ªèng c·∫•u tr√∫c ch·ªØ\n",
    "    denoised = cv2.medianBlur(binary, 3)\n",
    "\n",
    "    # 5. L√ÄM ƒê·∫¨M N√âT CH·ªÆ (Erosion)\n",
    "    # V·ªõi ·∫£nh n·ªÅn tr·∫Øng ch·ªØ ƒëen, ph√©p Erode s·∫Ω l√†m c√°c v√πng ƒëen (ch·ªØ) d√†y l√™n.\n",
    "    # ƒêi·ªÅu n√†y c·ª±c k·ª≥ quan tr·ªçng ƒë·ªÉ Tesseract nh·∫≠n di·ªán ƒë∆∞·ª£c c√°c ch·ªØ b·ªã nh·∫°t/ƒë·ª©t n√©t.\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    final_img = cv2.erode(denoised, kernel, iterations=1)\n",
    "\n",
    "    return final_img, img\n",
    "\n",
    "# =====================================================\n",
    "# 4. TH·ª∞C THI X·ª¨ L√ù ƒê·ªíNG LO·∫†T\n",
    "# =====================================================\n",
    "print(f\"üöÄ ƒêang x·ª≠ l√Ω ƒë·ªìng lo·∫°t...\")\n",
    "\n",
    "count = 0\n",
    "for file_name in target_files:\n",
    "    full_path = os.path.join(INPUT_DIR, file_name)\n",
    "    processed_img, _ = process_optimized_for_ocr(full_path)\n",
    "\n",
    "    if processed_img is not None:\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"processed_{file_name}\")\n",
    "        cv2.imwrite(output_path, processed_img)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\"‚úÖ ƒê√£ x·ª≠ l√Ω {count}/{len(target_files)} ·∫£nh...\")\n",
    "\n",
    "print(f\"\\n‚ú® Ho√†n t·∫•t! ƒê√£ l∆∞u {count} ·∫£nh v√†o: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6740e",
   "metadata": {},
   "source": [
    "3. X·ª≠ l√≠ c√°c ·∫£nh th√¥ng qua s·ª≠ d·ª•ng Paddle OCR v√† Teserract OCR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ec50430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-250.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-251.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-252.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-253.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-254.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-255.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-256.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-257.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-258.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-259.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-260.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-261.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-262.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-263.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-264.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-265.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-266.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-267.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-268.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-269.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-270.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-271.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-272.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-273.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-274.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-275.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-276.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-277.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-278.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-279.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-280.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-281.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-282.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-283.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-284.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-285.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-286.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-287.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-288.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-289.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-290.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-291.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-292.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-293.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-294.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-295.png\n",
      "ƒêang x·ª≠ l√Ω: ../image/PDF1_processed\\processed_PDF1-296.png\n",
      "Ho√†n th√†nh! ƒê√£ l∆∞u: ../OCR\\PDF1_626_750.csv\n"
     ]
    }
   ],
   "source": [
    "TESSERACT_LANGS = 'vie'\n",
    "\n",
    "IMG_DIR = \"../image/PDF1_processed\"\n",
    "\n",
    "OUT_DIR = \"../OCR\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "START_PAGE = 250\n",
    "\n",
    "END_PAGE = 296\n",
    "\n",
    "\n",
    "\n",
    "# Kh·ªüi t·∫°o PaddleOCR\n",
    "\n",
    "paddle_engine = PaddleOCR(use_angle_cls=True, lang='ch', show_log=False)\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "\n",
    "# 2. H√ÄM H·ªñ TR·ª¢\n",
    "\n",
    "# ===============================\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "\n",
    "    return \"\".join(\n",
    "\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def is_chinese(text: str) -> bool:\n",
    "\n",
    "    return any('\\u4e00' <= ch <= '\\u9fff' or '\\u3400' <= ch <= '\\u4DBF' for ch in text)\n",
    "\n",
    "\n",
    "\n",
    "def is_page_number_line(text: str) -> bool:\n",
    "\n",
    "    return re.fullmatch(r\"^\\s*\\d{1,4}\\s*$\", text) is not None\n",
    "\n",
    "\n",
    "\n",
    "def looks_like_vietnamese(text: str) -> bool:\n",
    "\n",
    "    norm = strip_accents(text).lower()\n",
    "\n",
    "    tokens = re.findall(r\"[a-z0-9]+\", norm)\n",
    "\n",
    "    def has_similar(target: str, thr: float) -> bool:\n",
    "\n",
    "        for t in tokens:\n",
    "\n",
    "            if difflib.SequenceMatcher(None, t, target).ratio() >= thr: return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    return (has_similar(\"buc\", 0.7) and has_similar(\"thu\", 0.7) and\n",
    "\n",
    "            has_similar(\"viet\", 0.7) and has_similar(\"cho\", 0.7))\n",
    "\n",
    "\n",
    "\n",
    "def extract_id(text: str):\n",
    "\n",
    "    def norm_digits(tok: str):\n",
    "\n",
    "        tok = tok.strip().replace('O', '0').replace('o', '0').replace('I', '1').replace('l', '1').replace('|', '1').replace('!', '1')\n",
    "\n",
    "        digits = \"\".join(re.findall(r\"\\d+\", tok))\n",
    "\n",
    "        if not digits or len(digits) < 1: return None, None\n",
    "\n",
    "        return digits, None\n",
    "\n",
    "\n",
    "\n",
    "    m_zh_full = re.search(r\"ÂÜô\\s*Áªô\\s*Ëá™Â∑±\\s*.*Á¨¨\\s*([0-9OoIl|!\\s]{1,20})\\s*Â∞Å\\s*‰ø°\", text)\n",
    "\n",
    "    if m_zh_full:\n",
    "\n",
    "        id_val, _ = norm_digits(m_zh_full.group(1))\n",
    "\n",
    "        if id_val: return id_val, 'ZH_FULL'\n",
    "\n",
    "\n",
    "\n",
    "    norm = strip_accents(text).lower()\n",
    "\n",
    "    m_vi_full = re.search(r\"buc\\s*thu\\s*viet\\s*cho\\s*chinh\\s*minh\\s*(?:so|s0|s6|s)\\s*([0-9OoIl|!\\s]{1,20})\", norm)\n",
    "\n",
    "    if m_vi_full:\n",
    "\n",
    "        id_val, _ = norm_digits(m_vi_full.group(1))\n",
    "\n",
    "        if id_val: return id_val, 'VI_FULL'\n",
    "\n",
    "\n",
    "\n",
    "    m_zh_anchor = re.search(r\"Á¨¨\\s*([0-9OoIl|!\\s]{1,20})\", text)\n",
    "\n",
    "    if m_zh_anchor:\n",
    "\n",
    "        id_val, _ = norm_digits(m_zh_anchor.group(1))\n",
    "\n",
    "        if id_val: return id_val, 'ZH_ANCHOR'\n",
    "\n",
    "\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "\n",
    "# 3. H√ÄM OCR K·∫æT H·ª¢P (HYBRID)\n",
    "\n",
    "# ===============================\n",
    "\n",
    "def hybrid_ocr(img_path: str):\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    results = paddle_engine.ocr(img_path, cls=True)\n",
    "\n",
    "    final_lines = []\n",
    "\n",
    "    if not results or results[0] is None: return []\n",
    "\n",
    "\n",
    "\n",
    "    for line in results[0]:\n",
    "\n",
    "        box = line[0]\n",
    "\n",
    "        paddle_text = line[1][0].strip()\n",
    "\n",
    "        if not is_chinese(paddle_text) and len(paddle_text) > 2:\n",
    "\n",
    "            try:\n",
    "\n",
    "                pts = np.array(box, dtype=np.int32)\n",
    "\n",
    "                x, y, w, h = cv2.boundingRect(pts)\n",
    "\n",
    "                roi = img[max(0, y-5):y+h+5, max(0, x-5):x+w+5]\n",
    "\n",
    "                roi_pil = Image.fromarray(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                tess_text = pytesseract.image_to_string(roi_pil, lang=TESSERACT_LANGS, config='--psm 7').strip()\n",
    "\n",
    "                final_lines.append(tess_text if len(tess_text) > 1 else paddle_text)\n",
    "\n",
    "            except:\n",
    "\n",
    "                final_lines.append(paddle_text)\n",
    "\n",
    "        else:\n",
    "\n",
    "            final_lines.append(paddle_text)\n",
    "\n",
    "    return final_lines\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "# 4. X·ª¨ L√ù D·ªÆ LI·ªÜU (C·∫¨P NH·∫¨T LOGIC CHUY·ªÇN TRANG)\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "DATA_MAP = defaultdict(lambda: {\"src\": [], \"tgt\": [], \"vi_started\": False})\n",
    "\n",
    "STATE = {\"current_id\": None, \"mode\": None, \"pending_src\": []}\n",
    "\n",
    "\n",
    "\n",
    "def process_image_to_data(img_path: str, data_map, state):\n",
    "\n",
    "    lines = hybrid_ocr(img_path)\n",
    "\n",
    "   \n",
    "\n",
    "    current_id = state[\"current_id\"]\n",
    "\n",
    "    mode = state[\"mode\"]\n",
    "\n",
    "    pending_src = state[\"pending_src\"]\n",
    "\n",
    "\n",
    "\n",
    "    for text in lines:\n",
    "\n",
    "        if is_page_number_line(text): continue\n",
    "\n",
    "       \n",
    "\n",
    "        found_id, id_type = extract_id(text)\n",
    "\n",
    "\n",
    "\n",
    "        # N·∫øu t√¨m th·∫•y ID m·ªõi (Ti√™u ƒë·ªÅ b√†i m·ªõi)\n",
    "\n",
    "        if found_id:\n",
    "\n",
    "            current_id = found_id\n",
    "\n",
    "            if id_type in ('ZH_FULL', 'ZH_ANCHOR'):\n",
    "\n",
    "                mode = \"zh\"\n",
    "\n",
    "                if pending_src:\n",
    "\n",
    "                    data_map[current_id][\"src\"].extend(pending_src)\n",
    "\n",
    "                    pending_src.clear()\n",
    "\n",
    "                data_map[current_id][\"src\"].append(text)\n",
    "\n",
    "                data_map[current_id][\"vi_started\"] = False\n",
    "\n",
    "            elif id_type in ('VI_FULL', 'VI_ANCHOR'):\n",
    "\n",
    "                mode = \"vi\"\n",
    "\n",
    "                data_map[current_id][\"vi_started\"] = True\n",
    "\n",
    "                data_map[current_id][\"tgt\"].append(text)\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        if current_id is None:\n",
    "\n",
    "            if is_chinese(text): pending_src.append(text)\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # --- LOGIC PH√ÇN LO·∫†I D√íNG TEXT ---\n",
    "\n",
    "        has_zh = is_chinese(text)\n",
    "\n",
    "        is_vi_hint = looks_like_vietnamese(text)\n",
    "\n",
    "\n",
    "\n",
    "        # 1. N·∫øu g·∫∑p d·∫•u hi·ªáu ti·∫øng Vi·ªát (B·ª©c th∆∞ vi·∫øt cho...) -> Chuy·ªÉn mode VI ngay\n",
    "\n",
    "        if is_vi_hint:\n",
    "\n",
    "            mode = \"vi\"\n",
    "\n",
    "            data_map[current_id][\"vi_started\"] = True\n",
    "\n",
    "            data_map[current_id][\"tgt\"].append(text)\n",
    "\n",
    "       \n",
    "\n",
    "        # 2. N·∫øu ƒëang ·ªü mode ti·∫øng Vi·ªát\n",
    "\n",
    "        elif mode == \"vi\":\n",
    "\n",
    "            # N·∫øu ƒëang VI m√† t·ª± d∆∞ng g·∫∑p ch·ªØ Trung -> Kh·∫£ nƒÉng l√† ID m·ªõi nh∆∞ng OCR ti√™u ƒë·ªÅ l·ªói\n",
    "\n",
    "            if has_zh:\n",
    "\n",
    "                mode = \"zh\"\n",
    "\n",
    "                data_map[current_id][\"src\"].append(text)\n",
    "\n",
    "            else:\n",
    "\n",
    "                data_map[current_id][\"tgt\"].append(text)\n",
    "\n",
    "\n",
    "\n",
    "        # 3. N·∫øu ƒëang ·ªü mode ti·∫øng Trung (ho·∫∑c Pinyin)\n",
    "\n",
    "        else: # mode == \"zh\"\n",
    "\n",
    "            if has_zh:\n",
    "\n",
    "                data_map[current_id][\"src\"].append(text)\n",
    "\n",
    "            else:\n",
    "\n",
    "                # N·∫øu kh√¥ng c√≥ ch·ªØ Trung: ki·ªÉm tra xem ƒë√£ b·∫Øt ƒë·∫ßu ph·∫ßn Vi·ªát ch∆∞a\n",
    "\n",
    "                if data_map[current_id][\"vi_started\"]:\n",
    "\n",
    "                    mode = \"vi\"\n",
    "\n",
    "                    data_map[current_id][\"tgt\"].append(text)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # V·∫´n coi l√† src (Pinyin) n·∫øu ch∆∞a c√≥ hint Vi·ªát r√µ r√†ng\n",
    "\n",
    "                    data_map[current_id][\"src\"].append(text)\n",
    "\n",
    "\n",
    "\n",
    "    # C·∫≠p nh·∫≠t tr·∫°ng th√°i sau khi xong 1 ·∫£nh\n",
    "\n",
    "    state[\"current_id\"] = current_id\n",
    "\n",
    "    state[\"mode\"] = mode\n",
    "\n",
    "    state[\"pending_src\"] = pending_src\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "# 5. CH·∫†Y V√Ä L∆ØU\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "for p in range(START_PAGE, END_PAGE + 1):\n",
    "\n",
    "    img_path = os.path.join(IMG_DIR, f\"processed_PDF1-{p}.png\")\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "\n",
    "        print(f\"ƒêang x·ª≠ l√Ω: {img_path}\")\n",
    "\n",
    "        process_image_to_data(img_path, DATA_MAP, STATE)\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for sid, v in DATA_MAP.items():\n",
    "\n",
    "    src = \" \".join(v[\"src\"]).strip()\n",
    "\n",
    "    tgt = \" \".join(v[\"tgt\"]).strip()\n",
    "\n",
    "    if src or tgt:\n",
    "\n",
    "        rows.append({\"src_id\": sid, \"src_lang\": src, \"tgt_lang\": tgt})\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "if not df.empty:\n",
    "\n",
    "    df['src_id_int'] = pd.to_numeric(df['src_id'], errors='coerce')\n",
    "\n",
    "    df = df.dropna(subset=['src_id_int'])\n",
    "\n",
    "    df = df[(df['src_id_int'] >= 626) & (df['src_id_int'] <= 750)]\n",
    "\n",
    "    df = df.sort_values('src_id_int').drop(columns=['src_id_int']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    out_csv = os.path.join(OUT_DIR, \"PDF1_626_750.csv\")\n",
    "\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Ho√†n th√†nh! ƒê√£ l∆∞u: {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c5aae",
   "metadata": {},
   "source": [
    "4. H·∫≠u x·ª≠ l√≠ OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae2994ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_src_lang(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "\n",
    "    # 1. X√≥a m·ªçi lo·∫°i d·∫•u nh√°y\n",
    "    text = re.sub(r'[\"\\'`‚Äú‚Äù‚Äò‚Äô‚Äü‚Äû¬´¬ª]', '', text)\n",
    "\n",
    "    # 2. FIX OCR: x√≥a k√Ω t·ª± r√°c ƒë·ª©ng ngay tr∆∞·ªõc ch·ªØ ÂÜô\n",
    "    text = re.sub(r'[È£û‰ª•,Ôºå]\\s*(?=ÂÜô)', '', text)\n",
    "\n",
    "    # 3. C·∫Øt t·ª´ ch·ªØ H√°n ƒë·∫ßu ti√™n\n",
    "    match_start = re.search(r'[\\u4e00-\\u9fff]', text)\n",
    "    if match_start:\n",
    "        text = text[match_start.start():]\n",
    "\n",
    "    # 4. C·∫Øt ph·∫ßn phi√™n √¢m (Pinyin)\n",
    "    match_pinyin = re.search(r'[a-zA-ZƒÅ√°«é√†ƒì√©ƒõ√®ƒ´√≠«ê√¨≈ç√≥«í√≤≈´√∫«î√π«ñ«ò«ö«ú]', text)\n",
    "    if match_pinyin:\n",
    "        text = text[:match_pinyin.start()]\n",
    "\n",
    "    # 5. X√≥a k√Ω t·ª± r√°c tr∆∞·ªõc & sau d·∫•u c√¢u ti·∫øng Trung\n",
    "    text = re.sub(r'[^„ÄÇ\\u4e00-\\u9fff]\\s*„ÄÇ', '„ÄÇ', text)\n",
    "    text = re.sub(r'„ÄÇ\\s*[^„ÄÇ\\u4e00-\\u9fff]+', '„ÄÇ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def clean_tgt_lang(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "\n",
    "    # 1. ƒê·ªìng b·ªô h√≥a: C·∫Øt t·ª´ c·ª•m \"B·ª©c th∆∞\"\n",
    "    marker = \"B·ª©c th∆∞\"\n",
    "    idx = text.find(marker)\n",
    "    if idx != -1:\n",
    "        text = text[idx:]\n",
    "\n",
    "    # 2. X√ìA TRI·ªÜT ƒê·ªÇ D·∫§U NH√ÅY (bao g·ªìm c·∫£ nh√°y ƒë∆°n ' v√† c√°c lo·∫°i nh√°y l·∫°)\n",
    "    text = re.sub(r'[\"\\'`‚Äú‚Äù‚Äò‚Äô‚Äü‚Äû¬´¬ª]', '', text)\n",
    "\n",
    "    # 3. X√ìA K√ù T·ª∞ R√ÅC ASCII\n",
    "    # Lo·∫°i b·ªè c√°c k√Ω t·ª± nh∆∞: | ~ * ^ % $ # @ + = < > / \\ _\n",
    "    text = re.sub(r'[|~*^#$%@\\+=\\[\\]{}<>/\\\\]', '', text)\n",
    "\n",
    "    # 4. L√ÄM S·∫†CH D·∫§U C√ÇU TI·∫æNG VI·ªÜT\n",
    "    # X√≥a kho·∫£ng tr·∫Øng th·ª´a tr∆∞·ªõc d·∫•u ch·∫•m, ph·∫©y\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    # X√≥a c√°c k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ/s·ªë d√≠nh s√°t v√†o d·∫•u ch·∫•m\n",
    "    text = re.sub(r'[^.\\w√Ä-·ªπ\\d\\s](?=\\.)', '', text)\n",
    "    text = re.sub(r'(?<=\\.)[^.\\w√Ä-·ªπ\\d\\s]', '', text)\n",
    "\n",
    "    # 5. CHU·∫®N H√ìA KHO·∫¢NG TR·∫ÆNG\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# ===============================\n",
    "# QUY TR√åNH X·ª¨ L√ù FILE\n",
    "# ===============================\n",
    "input_file = '../OCR/PDF1_626_750.csv'\n",
    "output_file = '../OCR/PDF1_626_750_cleaned.csv'\n",
    "\n",
    "try:\n",
    "    # ƒê·ªçc t·ª´ file g·ªëc\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # √Åp d·ª•ng c√°c h√†m l√†m s·∫°ch\n",
    "    df['src_lang'] = df['src_lang'].apply(clean_src_lang)\n",
    "    df['tgt_lang'] = df['tgt_lang'].apply(clean_tgt_lang)\n",
    "\n",
    "    # L∆∞u v√†o file cleaned\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig') \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n: {input_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
