{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d449b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import thư viện cần thiết\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from paddleocr import PaddleOCR\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from sacrebleu.metrics import BLEU\n",
    "from bert_score import score\n",
    "from googletrans import Translator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54cf5f",
   "metadata": {},
   "source": [
    "1. Chuyển file PDF sang các file ảnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58d089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xử lí PDF 1 thành hình ảnh dạng png từ trang 203 đến trang 395 (theo id)\n",
    "pdf_file = \"../data/pdf/PDF1.pdf\"\n",
    "output_folder = \"../image/PDF1\"\n",
    "\n",
    "\n",
    "images = convert_from_path(\n",
    "    pdf_file,\n",
    "    first_page=203,\n",
    "    last_page=395\n",
    ")\n",
    "\n",
    "for page_num, img in enumerate(images, start=203):\n",
    "    filename = f\"PDF1-{page_num}.png\"\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    img.save(save_path, \"PNG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d829e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xử lí PDF 3 thành hình ảnh dạng png từ trang 12 đến trang 118 (theo id)\n",
    "pdf_file = \"../data/pdf/PDF3.pdf\"\n",
    "output_folder = \"../image/PDF3\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "start_page = 12\n",
    "end_page = 118\n",
    "\n",
    "images = convert_from_path(pdf_file, first_page=start_page, last_page=end_page)\n",
    "\n",
    "for page_num, img in zip(range(start_page, end_page + 1), images):\n",
    "    page_str = f\"{page_num:03d}\"           \n",
    "    filename = f\"PDF3-{page_str}.png\"\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    img.save(save_path, \"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f6eff",
   "metadata": {},
   "source": [
    "2. Tiền xử lí ảnh trước khi OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6752acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Đã xử lý 10/47 ảnh\n",
      " Đã xử lý 20/47 ảnh\n",
      " Đã xử lý 30/47 ảnh\n",
      " Đã xử lý 40/47 ảnh\n",
      "\n",
      "  Đã lưu 47 ảnh vào: ../image\\PDF1_processed\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "INPUT_ROOT = '../image'\n",
    "INPUT_SUBDIR = 'PDF1'\n",
    "INPUT_DIR = os.path.join(INPUT_ROOT, INPUT_SUBDIR)\n",
    "\n",
    "OUTPUT_SUBDIR = 'PDF1_processed'\n",
    "OUTPUT_DIR = os.path.join(INPUT_ROOT, OUTPUT_SUBDIR)\n",
    "\n",
    "START_PAGE = 250\n",
    "END_PAGE = 296\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"✅ Đã tạo thư mục đầu ra: {OUTPUT_DIR}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. LỌC FILE THEO SỐ TRANG (GIỮ NGUYÊN)\n",
    "# =====================================================\n",
    "target_files = []\n",
    "pattern = re.compile(r'PDF1-(\\d+)\\.png$', re.IGNORECASE)\n",
    "\n",
    "try:\n",
    "    all_files = os.listdir(INPUT_DIR)\n",
    "    for file_name in all_files:\n",
    "        match = pattern.match(file_name)\n",
    "        if match:\n",
    "            page_number = int(match.group(1))\n",
    "            if START_PAGE <= page_number <= END_PAGE:\n",
    "                target_files.append(file_name)\n",
    "except FileNotFoundError:\n",
    "    print(f\" Không tìm thấy thư mục đầu vào: {INPUT_DIR}\")\n",
    "\n",
    "target_files.sort(key=lambda f: int(pattern.search(f).group(1)))\n",
    "\n",
    "# =====================================================\n",
    "# 3. HÀM XỬ LÝ TỐI ƯU OCR: CHỐNG MẤT NÉT & LÀM ĐẬM CHỮ\n",
    "# =====================================================\n",
    "def process_optimized_for_ocr(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None, None\n",
    "\n",
    "    # 1. Chuyển sang Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. Tăng cường độ tương phản cục bộ (CLAHE)\n",
    "    # Giúp các dòng chữ nhạt màu như \"số 641\" hiện rõ hơn khỏi nền\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "\n",
    "    # 3. Adaptive Thresholding (Phân ngưỡng thích nghi)\n",
    "    # Thay vì dùng ngưỡng cố định cho cả ảnh, nó tính toán từng vùng nhỏ\n",
    "    # giúp giữ lại nét chữ mảnh cực tốt.\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        enhanced, 255, \n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 21, 15\n",
    "    )\n",
    "\n",
    "    # 4. KHỬ NHIỄU NHẸ (Dùng Median Blur kích thước nhỏ nhất)\n",
    "    # Để xóa các đốm đen li ti mà không làm hỏng cấu trúc chữ\n",
    "    denoised = cv2.medianBlur(binary, 3)\n",
    "\n",
    "    # 5. LÀM ĐẬM NÉT CHỮ (Erosion)\n",
    "    # Với ảnh nền trắng chữ đen, phép Erode sẽ làm các vùng đen (chữ) dày lên.\n",
    "    # Điều này cực kỳ quan trọng để Tesseract nhận diện được các chữ bị nhạt/đứt nét.\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    final_img = cv2.erode(denoised, kernel, iterations=1)\n",
    "\n",
    "    return final_img, img\n",
    "\n",
    "# =====================================================\n",
    "# 4. THỰC THI XỬ LÝ ĐỒNG LOẠT\n",
    "# =====================================================\n",
    "\n",
    "count = 0\n",
    "for file_name in target_files:\n",
    "    full_path = os.path.join(INPUT_DIR, file_name)\n",
    "    processed_img, _ = process_optimized_for_ocr(full_path)\n",
    "\n",
    "    if processed_img is not None:\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"processed_{file_name}\")\n",
    "        cv2.imwrite(output_path, processed_img)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print(f\" Đã xử lý {count}/{len(target_files)} ảnh\")\n",
    "\n",
    "print(f\"\\n  Đã lưu {count} ảnh vào: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da6740e",
   "metadata": {},
   "source": [
    "3. Xử lí các ảnh thông qua sử dụng Paddle OCR và Teserract OCR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99267634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR Tesseract\n",
    "TESSERACT_LANGS = 'chi_sim+chi_tra+vie'\n",
    "def tesseract_ocr(img_path: str, lang: str = TESSERACT_LANGS):\n",
    "    try:\n",
    "        text_raw = pytesseract.image_to_string(\n",
    "            Image.open(img_path),\n",
    "            lang=lang,\n",
    "            config='--psm 3'\n",
    "        )\n",
    "        return [l.strip() for l in text_raw.split('\\n') if l.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi OCR {img_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "# OCR Paddle\n",
    "ocr_paddle = PaddleOCR(\n",
    "    use_angle_cls=True,\n",
    "    lang='ch',\n",
    "    show_log=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a86760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def is_chinese(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "        \n",
    "    pattern = (\n",
    "        r'^['\n",
    "        # Nhóm Hán tự\n",
    "        r'\\u4E00-\\u9FFF'  # CJK Unified Ideographs (Phổ thông)\n",
    "        r'\\u3400-\\u4DBF'  # CJK Extension A\n",
    "        r'\\uF900-\\uFAFF'  # CJK Compatibility\n",
    "        \n",
    "        # Nhóm dấu câu CJK\n",
    "        r'\\u3000-\\u303F'  # Dấu câu TQ (。 、 【 】...)\n",
    "        r'\\uFF00-\\uFFEF'  # Dấu câu Full-width (！, ？, ０-９...)\n",
    "        r'\\uFE30-\\uFE4F'  # CJK Compatibility Forms\n",
    "        \n",
    "        # Nhóm ASCII (không có kí tự Latin)\n",
    "        r'\\u0020-\\u0040'  # Khoảng trắng, Dấu câu (!..@), SỐ (0-9)\n",
    "        r'\\u005B-\\u0060'  # Dấu câu ( [ .. ` )\n",
    "        r'\\u007B-\\u007E'  # Dấu câu ( { .. ~ )\n",
    "        \n",
    "        # Nhóm Latin-1 Suplement (Giữ lại để bắt dấu » « ©)\n",
    "        # r'\\u00A0-\\u00FF'  \n",
    "        r']+$'\n",
    "    )\n",
    "    \n",
    "    return bool(re.fullmatch(pattern, text))\n",
    "\n",
    "def is_page_number_line(text: str) -> bool:\n",
    "    return re.fullmatch(r\"^\\s*\\d{1,4}\\s*$\", text) is not None\n",
    "\n",
    "def looks_like_vietnamese(text: str) -> bool:\n",
    "    norm = strip_accents(text).lower()\n",
    "    tokens = re.findall(r\"[a-z0-9]+\", norm)\n",
    "\n",
    "    def has_similar(target: str, thr: float) -> bool:\n",
    "        for t in tokens:\n",
    "            if difflib.SequenceMatcher(None, t, target).ratio() >= thr:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    return (\n",
    "        has_similar(\"buc\", 0.5) and\n",
    "        has_similar(\"thu\", 0.5) and\n",
    "        has_similar(\"viet\", 0.5) and\n",
    "        has_similar(\"cho\", 0.5) and\n",
    "        has_similar(\"chinh\", 0.5) and\n",
    "        has_similar(\"minh\", 0.5)\n",
    "    )\n",
    "\n",
    "def extract_id(text: str):\n",
    "    def norm_digits(tok: str):\n",
    "        tok = tok.strip()\n",
    "        tok = tok.replace('O', '0').replace('o', '0')\n",
    "        tok = tok.replace('I', '1').replace('l', '1').replace('|', '1').replace('!', '1')\n",
    "\n",
    "        digits = \"\".join(re.findall(r\"\\d+\", tok))  # \"50 4\" -> \"504\"\n",
    "        if not digits:\n",
    "            return None\n",
    "        if len(digits) < 3:\n",
    "            return None\n",
    "        return digits\n",
    "\n",
    "    # Chinese: 第xxx \n",
    "    m = re.search(r\"第\\s*([0-9OoIl|!\\s]{1,20})\", text)\n",
    "    if m:\n",
    "        return norm_digits(m.group(1))\n",
    "\n",
    "    # Vietnamese: so/s0/s6 ...\n",
    "    norm = strip_accents(text).lower()\n",
    "    m = re.search(r\"(?:\\bso\\b|\\bs0\\b|\\bs6\\b|s[0o6])\\s*([0-9OoIl|!\\s]{1,20})\", norm)\n",
    "    if m:\n",
    "        return norm_digits(m.group(1))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8828b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse ảnh -> Data\n",
    "DATA_MAP_TESSERACT = defaultdict(lambda: {\"src\": [], \"tgt\": [], \"vi_started\": False})\n",
    "DATA_MAP_PADDLE = defaultdict(lambda: {\"src\": [], \"tgt\": [], \"vi_started\": False})\n",
    "STATE_TESSERACT = {\"current_id\": None, \"mode\": None, \"pending_src\": []}\n",
    "STATE_PADDLE = {\"current_id\": None, \"mode\": None, \"pending_src\": []}\n",
    "\n",
    "def process_image_to_data(img_path: str, data_map, state, ocr_model):\n",
    "    if ocr_model == 'Tesseract':\n",
    "        lines = tesseract_ocr(img_path)\n",
    "    else:\n",
    "        result = ocr_paddle.ocr(img_path, cls=True)\n",
    "        if not result or result[0] is None:\n",
    "            return\n",
    "\n",
    "        blocks = result[0]\n",
    "        blocks = sorted(\n",
    "            blocks,\n",
    "            key=lambda b: (min(p[1] for p in b[0]), min(p[0] for p in b[0]))\n",
    "        )\n",
    "        lines = [b[1][0].strip() for b in blocks if b and b[1] and b[1][0]]\n",
    "\n",
    "    current_id = state[\"current_id\"]\n",
    "    mode = state[\"mode\"]\n",
    "    pending_src = state[\"pending_src\"]\n",
    "\n",
    "    for text in lines:\n",
    "        if not text:\n",
    "            continue\n",
    "        if is_page_number_line(text):\n",
    "            continue\n",
    "        \n",
    "        # print(f\"Model: {ocr_model}\\n{text}\")\n",
    "\n",
    "        # Title tiếng Việt -> mode=vi (lấy ID) và chỉ xử lý như title nếu extract được ID\n",
    "        if looks_like_vietnamese(text):\n",
    "            maybe_id = extract_id(text)\n",
    "            if maybe_id:\n",
    "                # Title thật\n",
    "                current_id = maybe_id\n",
    "                _ = data_map[current_id]\n",
    "                if pending_src:\n",
    "                    data_map[current_id][\"src\"].extend(pending_src)\n",
    "                    pending_src.clear()\n",
    "\n",
    "                mode = \"vi\"\n",
    "                if not data_map[current_id][\"vi_started\"]:\n",
    "                    data_map[current_id][\"tgt\"] = []\n",
    "                    data_map[current_id][\"vi_started\"] = True\n",
    "                \n",
    "                data_map[current_id][\"tgt\"].append(\n",
    "                    f\"Bức thư viết cho chính mình số {current_id}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        # Anchor ID (ưu tiên từ tiếng Trung: 第...)\n",
    "        found_id = extract_id(text)\n",
    "        if found_id:\n",
    "            current_id = found_id\n",
    "            _ = data_map[current_id]\n",
    "            if pending_src:\n",
    "                data_map[current_id][\"src\"].extend(pending_src)\n",
    "                pending_src.clear()\n",
    "\n",
    "            mode = \"zh\" if is_chinese(text) else \"vi\"\n",
    "            if mode == \"vi\" and (not data_map[current_id][\"vi_started\"]):\n",
    "                data_map[current_id][\"tgt\"] = []\n",
    "                data_map[current_id][\"vi_started\"] = True\n",
    "            \n",
    "            data_map[current_id][\"src\"].append(\n",
    "                f\"写 给 自己 的 第 {current_id} 封 信\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Chưa có ID -> giữ tiếng Trung để chờ, bỏ qua Pinyin\n",
    "        if current_id is None:\n",
    "            if is_chinese(text):\n",
    "                pending_src.append(text)\n",
    "            continue\n",
    "\n",
    "        # Gom nội dung theo mode\n",
    "        if is_chinese(text):\n",
    "            if mode == \"vi\":\n",
    "                # Gặp tiếng Trung sau tiếng Việt -> Reset để chờ ID mới\n",
    "                pending_src.append(text)\n",
    "                current_id = None\n",
    "                mode = \"zh\"\n",
    "            else:\n",
    "                data_map[current_id][\"src\"].append(text)\n",
    "        else:\n",
    "            if mode == \"vi\":\n",
    "                data_map[current_id][\"tgt\"].append(text)\n",
    "\n",
    "    state[\"current_id\"] = current_id\n",
    "    state[\"mode\"] = mode\n",
    "    state[\"pending_src\"] = pending_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7cbcb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý: ../image\\PDF1/PDF1-250.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-251.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-252.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-253.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-254.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-255.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-256.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-257.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-258.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-259.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-260.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-261.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-262.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-263.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-264.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-265.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-266.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-267.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-268.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-269.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-270.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-271.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-272.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-273.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-274.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-275.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-276.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-277.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-278.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-279.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-280.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-281.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-282.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-283.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-284.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-285.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-286.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-287.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-288.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-289.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-290.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-291.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-292.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-293.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-294.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-295.png...\n",
      "Đang xử lý: ../image\\PDF1/PDF1-296.png...\n"
     ]
    }
   ],
   "source": [
    "# Main process\n",
    "IMG_DIR = \"../image\"\n",
    "OUT_DIR = \"../OCR\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "for p in range(START_PAGE, END_PAGE + 1):\n",
    "    img_path_tesseract = os.path.join(IMG_DIR, f\"{INPUT_SUBDIR}_processed/processed_PDF1-{p}.png\")\n",
    "    img_path_paddle = os.path.join(IMG_DIR, f\"{INPUT_SUBDIR}/PDF1-{p}.png\")\n",
    "    if os.path.exists(img_path_paddle):\n",
    "        print(f\"Đang xử lý: {img_path_paddle}...\")\n",
    "        process_image_to_data(img_path_tesseract, DATA_MAP_TESSERACT, STATE_TESSERACT, 'Tesseract')\n",
    "        process_image_to_data(img_path_paddle, DATA_MAP_PADDLE, STATE_PADDLE, 'Paddle')\n",
    "\n",
    "rows = []\n",
    "all_ids = set(DATA_MAP_PADDLE.keys()) | set(DATA_MAP_TESSERACT.keys())\n",
    "\n",
    "def sort_key(x):\n",
    "    try:\n",
    "        return int(str(x))\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "for sid in sorted(all_ids, key=sort_key):\n",
    "    # Lấy Tiếng Trung (src) từ PADDLE Map\n",
    "    paddle_data = DATA_MAP_PADDLE.get(sid, {\"src\": [], \"tgt\": []})\n",
    "    src_text = \" \".join(paddle_data[\"src\"]).strip()\n",
    "    \n",
    "    # Lấy Tiếng Việt (tgt) từ TESSERACT Map\n",
    "    tesseract_data = DATA_MAP_TESSERACT.get(sid, {\"src\": [], \"tgt\": []})\n",
    "    tgt_text = \" \".join(tesseract_data[\"tgt\"]).strip()\n",
    "    # print(f\"{sid}\\n\")\n",
    "    # print(f\"{src_text}\\n\")\n",
    "    # print(f\"{tgt_text}\\n\")\n",
    "    if src_text or tgt_text:\n",
    "        rows.append({\n",
    "            \"src_id\": sid,\n",
    "            \"src_lang\": src_text,  # src: Paddle\n",
    "            \"tgt_lang\": tgt_text   # tgt: Tesseract\n",
    "        })\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Lọc ID \n",
    "ID_START = 626\n",
    "ID_END = 750\n",
    "df = df[df['src_id'].str.isdigit()].copy()\n",
    "df['src_id_int'] = df['src_id'].astype(int)\n",
    "df = df[(df['src_id_int'] >= ID_START) & (df['src_id_int'] <= ID_END)]\n",
    "df = df.sort_values('src_id_int').drop(columns=['src_id_int']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c5aae",
   "metadata": {},
   "source": [
    "4. Hậu xử lí OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d0ecf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu vào: ../OCR\\PDF1_626_750.csv\n"
     ]
    }
   ],
   "source": [
    "# Định nghĩa bộ lọc Regex\n",
    "def is_vietnamese(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return False\n",
    "    \n",
    "    # \\u0020-\\u007E: Bao gồm a-z, A-Z, 0-9, dấu câu chuẩn (!, @, #, ., ?, ...)\n",
    "    # Phần còn lại: Các nguyên âm có dấu và chữ Đ/đ của tiếng Việt\n",
    "    vietnamese_chars = (\n",
    "        \"àáạảãâầấậẩẫăằắặẳẵ\"\n",
    "        \"èéẹẻẽêềếệểễ\"\n",
    "        \"ìíịỉĩ\"\n",
    "        \"òóọỏõôồốộổỗơờớợởỡ\"\n",
    "        \"ùúụủũưừứựửữ\"\n",
    "        \"ỳýỵỷỹ\"\n",
    "        \"đ\"\n",
    "        \"ÀÁẠẢÃÂẦẤẬẨẪĂẰẮẶẲẴ\"\n",
    "        \"ÈÉẸẺẼÊỀẾỆỂỄ\"\n",
    "        \"ÌÍỊỈĨ\"\n",
    "        \"ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ\"\n",
    "        \"ÙÚỤỦŨƯỪỨỰỬỮ\"\n",
    "        \"ỲÝỴỶỸ\"\n",
    "        \"Đ\"\n",
    "    )\n",
    "    \n",
    "    # Logic: Chuỗi hợp lệ chỉ được chứa các ký tự nằm trong 2 nhóm này\n",
    "    # LƯU Ý: Nếu văn bản của bạn có dấu nháy Unicode (“ ”), bạn phải thêm chúng vào pattern này\n",
    "    # hoặc xử lý chúng trước khi lọc, nếu không dòng đó sẽ bị False.\n",
    "    pattern = f'^[\\u0020-\\u007E{vietnamese_chars}]+$'\n",
    "    \n",
    "    return bool(re.match(pattern, text))\n",
    "\n",
    "# --- BỔ SUNG LOGIC XÓA TRƯỚC CHỮ 写 ---\n",
    "def trim_before_xie(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    idx = text.find('写')\n",
    "    if idx != -1:\n",
    "        return text[idx:]\n",
    "    return text\n",
    "\n",
    "# Áp dụng xóa ký tự trước chữ '写' cho cột src_lang\n",
    "df['src_lang'] = df['src_lang'].apply(trim_before_xie)\n",
    "\n",
    "# ---------------------------------------\n",
    "# ĐOẠN CODE GỐC CỦA BẠN TIẾP TỤC TẠI ĐÂY\n",
    "# ---------------------------------------\n",
    "\n",
    "# Drop các dòng bị thiếu (NaN) hoặc rỗng ở 1 trong 2 cột\n",
    "# Chuyển chuỗi chỉ có khoảng trắng thành NaN \n",
    "df = df.replace(r'^\\s*$', float('nan'), regex=True)\n",
    "df.dropna(subset=['src_lang', 'tgt_lang'], inplace=True)\n",
    "\n",
    "# Logic: Giữ lại dòng khi (Check Trung == True) và (Check Việt == True)\n",
    "# Các dòng False sẽ bị loại bỏ\n",
    "df_clean = df[\n",
    "    df['src_lang'].apply(is_chinese) & \n",
    "    df['tgt_lang'].apply(is_vietnamese)\n",
    "].copy()\n",
    "df_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Xuất CSV\n",
    "if not df_clean.empty:\n",
    "    min_id = df_clean['src_id'].astype(int).min()\n",
    "    max_id = df_clean['src_id'].astype(int).max()\n",
    "else:\n",
    "    min_id = max_id = 0\n",
    "\n",
    "out_csv = os.path.join(OUT_DIR, f\"PDF1_{min_id}_{max_id}.csv\")\n",
    "df_clean.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Đã lưu vào:\", out_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8af4c2",
   "metadata": {},
   "source": [
    "5. Alignment cho PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05aa5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = 'PDF1'\n",
    "FROM_ID = 626\n",
    "TO_ID = 750\n",
    "# INPUT_FILE = f\"../data/preprocessing_data/{SRC.lower()}_{FROM_ID}_{TO_ID}.csv\"\n",
    "INPUT_FILE = f\"../OCR/{SRC}_{FROM_ID}_{TO_ID}.csv\"\n",
    "OUTPUT_FILE = f\"../Alignment/{SRC}_{FROM_ID}_{TO_ID}.csv\"\n",
    "ALIGN_THRESHOLD = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0249635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    return 'cpu'\n",
    "\n",
    "def split_sentences(text, lang='vi'):\n",
    "    if not text: return []\n",
    "    if lang == 'zh':\n",
    "        sents = re.split(r'(?<=[。！？])\\s*', text)\n",
    "    else:\n",
    "        sents = re.split(\n",
    "            r'(?<!\\d\\.)(?<=[.?!])\\s+(?=(?:[A-ZÀ-Ỵ\"\\'(]|\\d{1,3}\\.))',\n",
    "            text\n",
    "        )\n",
    "\n",
    "    return [s.strip() for s in sents]\n",
    "\n",
    "def align_n3(src_sents, tgt_sents, model, device, threshold):\n",
    "    if not src_sents or not tgt_sents:\n",
    "        return []\n",
    "\n",
    "    # Encode\n",
    "    # 1-gram (Câu đơn)\n",
    "    emb_src = model.encode(src_sents, convert_to_tensor=True, device=device)\n",
    "    emb_tgt = model.encode(tgt_sents, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    # 2-gram (Gộp 2 câu)\n",
    "    tgt_merged_2 = [tgt_sents[i] + \" \" + tgt_sents[i+1] for i in range(len(tgt_sents)-1)]\n",
    "    src_merged_2 = [src_sents[i] + src_sents[i+1] for i in range(len(src_sents)-1)]\n",
    "    \n",
    "    emb_tgt_2 = model.encode(tgt_merged_2, convert_to_tensor=True, device=device) if tgt_merged_2 else None\n",
    "    emb_src_2 = model.encode(src_merged_2, convert_to_tensor=True, device=device) if src_merged_2 else None\n",
    "\n",
    "    # 3-gram (Gộp 3 câu) \n",
    "    tgt_merged_3 = [tgt_sents[i] + \" \" + tgt_sents[i+1] + \" \" + tgt_sents[i+2] for i in range(len(tgt_sents)-2)]\n",
    "    src_merged_3 = [src_sents[i] + src_sents[i+1] + src_sents[i+2] for i in range(len(src_sents)-2)]\n",
    "    \n",
    "    emb_tgt_3 = model.encode(tgt_merged_3, convert_to_tensor=True, device=device) if tgt_merged_3 else None\n",
    "    emb_src_3 = model.encode(src_merged_3, convert_to_tensor=True, device=device) if src_merged_3 else None\n",
    "\n",
    "    # Tính ma trận tương đồng (similarity matrices)\n",
    "    # Khởi tạo ma trận (rỗng chiều)\n",
    "    def get_sim(emb1, emb2):\n",
    "        if emb1 is not None and emb2 is not None:\n",
    "            return util.cos_sim(emb1, emb2).cpu().numpy()\n",
    "        return np.zeros((0,0))\n",
    "\n",
    "    sim_1_1 = get_sim(emb_src, emb_tgt)\n",
    "    \n",
    "    sim_1_2 = get_sim(emb_src, emb_tgt_2)     # Src đơn vs Tgt gộp 2\n",
    "    sim_2_1 = get_sim(emb_src_2, emb_tgt)     # Src gộp 2 vs Tgt đơn\n",
    "    \n",
    "    sim_1_3 = get_sim(emb_src, emb_tgt_3)     # Src đơn vs Tgt gộp 3 \n",
    "    sim_3_1 = get_sim(emb_src_3, emb_tgt)     # Src gộp 3 vs Tgt đơn\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Đánh dấu câu đã dùng\n",
    "    used_src = np.zeros(len(src_sents), dtype=bool)\n",
    "    used_tgt = np.zeros(len(tgt_sents), dtype=bool)\n",
    "\n",
    "    # Greedy search\n",
    "    while True:\n",
    "        best_score = -1\n",
    "        best_type = None\n",
    "        best_indices = None # (r, c)\n",
    "\n",
    "        # Helper để check max score trong matrix\n",
    "        def check_matrix(matrix, label):\n",
    "            nonlocal best_score, best_type, best_indices\n",
    "            if matrix.size > 0:\n",
    "                r, c = np.unravel_index(matrix.argmax(), matrix.shape)\n",
    "                if matrix[r, c] > best_score:\n",
    "                    best_score = matrix[r, c]\n",
    "                    best_type = label\n",
    "                    best_indices = (r, c)\n",
    "\n",
    "        # Kiểm tra tất cả các trường hợp\n",
    "        check_matrix(sim_1_1, '1-1')\n",
    "        check_matrix(sim_1_2, '1-2')\n",
    "        check_matrix(sim_2_1, '2-1')\n",
    "        check_matrix(sim_1_3, '1-3') \n",
    "        check_matrix(sim_3_1, '3-1') \n",
    "\n",
    "        if best_score < threshold:\n",
    "            break\n",
    "\n",
    "        r, c = best_indices\n",
    "        valid_match = False\n",
    "        \n",
    "        # Logic kết quả và masking\n",
    "        # Logic chung: Kiểm tra used -> Add result -> Mark used -> Mask matrix\n",
    "        \n",
    "        if best_type == '1-1':\n",
    "            if not used_src[r] and not used_tgt[c]:\n",
    "                results.append({'src': src_sents[r], 'tgt': tgt_sents[c], 'type': '1-1'})\n",
    "                used_src[r] = True; used_tgt[c] = True\n",
    "                valid_match = True\n",
    "                # Masking: Xóa hàng r (src) và cột c (tgt) ở mọi matrix liên quan: gán -1 trực tiếp vào các vùng ảnh hưởng\n",
    "                sim_1_1[r, :] = -1; sim_1_1[:, c] = -1\n",
    "                if sim_1_2.size: sim_1_2[r, :] = -1; sim_1_2[:, max(0, c-1):c+1] = -1\n",
    "                if sim_2_1.size: sim_2_1[max(0, r-1):r+1, :] = -1; sim_2_1[:, c] = -1\n",
    "                if sim_1_3.size: sim_1_3[r, :] = -1; sim_1_3[:, max(0, c-2):c+1] = -1\n",
    "                if sim_3_1.size: sim_3_1[max(0, r-2):r+1, :] = -1; sim_3_1[:, c] = -1\n",
    "\n",
    "        elif best_type == '1-2': # Src r vs Tgt c, c+1\n",
    "            if not used_src[r] and not used_tgt[c] and not used_tgt[c+1]:\n",
    "                results.append({'src': src_sents[r], 'tgt': tgt_sents[c] + \" \" + tgt_sents[c+1], 'type': '1-2'})\n",
    "                used_src[r] = True; used_tgt[c:c+2] = True\n",
    "                valid_match = True\n",
    "                sim_1_2[r, c] = -1 # Xóa chính nó\n",
    "                sim_1_1[r, :] = -1; sim_1_1[:, c:c+2] = -1\n",
    "               \n",
    "\n",
    "        elif best_type == '2-1': # Src r, r+1 vs Tgt c\n",
    "            if not used_src[r] and not used_src[r+1] and not used_tgt[c]:\n",
    "                results.append({'src': src_sents[r]+src_sents[r+1], 'tgt': tgt_sents[c], 'type': '2-1'})\n",
    "                used_src[r:r+2] = True; used_tgt[c] = True\n",
    "                valid_match = True\n",
    "                sim_2_1[r, c] = -1\n",
    "                sim_1_1[r:r+2, :] = -1; sim_1_1[:, c] = -1\n",
    "\n",
    "        elif best_type == '1-3': # Src r vs Tgt c, c+1, c+2\n",
    "            if not used_src[r] and not used_tgt[c] and not used_tgt[c+1] and not used_tgt[c+2]:\n",
    "                results.append({'src': src_sents[r], 'tgt': tgt_sents[c]+\" \"+tgt_sents[c+1]+\" \"+tgt_sents[c+2], 'type': '1-3'})\n",
    "                used_src[r] = True; used_tgt[c:c+3] = True\n",
    "                valid_match = True\n",
    "                sim_1_3[r, c] = -1\n",
    "                sim_1_1[r, :] = -1; sim_1_1[:, c:c+3] = -1\n",
    "\n",
    "        elif best_type == '3-1': # Src r, r+1, r+2 vs Tgt c\n",
    "            if not used_src[r] and not used_src[r+1] and not used_src[r+2] and not used_tgt[c]:\n",
    "                results.append({'src': src_sents[r]+src_sents[r+1]+src_sents[r+2], 'tgt': tgt_sents[c], 'type': '3-1'})\n",
    "                used_src[r:r+3] = True; used_tgt[c] = True\n",
    "                valid_match = True\n",
    "                sim_3_1[r, c] = -1\n",
    "                sim_1_1[r:r+3, :] = -1; sim_1_1[:, c] = -1\n",
    "\n",
    "        # Nếu không match được (do conflict đã bị dùng) -> xóa điểm này để tìm max tiếp theo\n",
    "        if not valid_match:\n",
    "            if best_type == '1-1': sim_1_1[r, c] = -1\n",
    "            elif best_type == '1-2': sim_1_2[r, c] = -1\n",
    "            elif best_type == '2-1': sim_2_1[r, c] = -1\n",
    "            elif best_type == '1-3': sim_1_3[r, c] = -1\n",
    "            elif best_type == '3-1': sim_3_1[r, c] = -1\n",
    "\n",
    "        # Cập nhật kết quả\n",
    "        if valid_match:\n",
    "            results[-1]['score'] = float(best_score)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d1a6bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải model LaBSE trên cpu (n=3)...\n",
      "Bắt đầu Alignment (1-1, 1-2, 2-1, 1-3, 3-1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [02:20<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả Alignment\n",
      "Tổng số cặp: 125\n",
      "Phân bố cặp câu: type\n",
      "1-1    45\n",
      "1-2    32\n",
      "2-1    28\n",
      "1-3    17\n",
      "3-1     3\n",
      "Name: count, dtype: int64\n",
      "Average Semantic Score: 0.8045\n",
      "Đã lưu file: ../Alignment/PDF1_626_750.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(f\"Đang tải model LaBSE trên {device} (n=3)...\")\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE', device=device)\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df[\"src_lang\"] = df[\"src_lang\"].fillna(\"\").astype(str)\n",
    "df[\"tgt_lang\"] = df[\"tgt_lang\"].fillna(\"\").astype(str)\n",
    "\n",
    "final_data = []\n",
    "\n",
    "print(\"Bắt đầu Alignment (1-1, 1-2, 2-1, 1-3, 3-1)...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    src_sents = split_sentences(row.get('src_lang', ''), lang='zh')\n",
    "    tgt_sents = split_sentences(row.get('tgt_lang', ''), lang='vi')\n",
    "    pairs = align_n3(src_sents, tgt_sents, model, device, ALIGN_THRESHOLD)\n",
    "    \n",
    "    for p in pairs:\n",
    "        final_data.append({\n",
    "            'src_id': row.get('src_id', ''),\n",
    "            'src_lang': p['src'],\n",
    "            'tgt_lang': p['tgt'],\n",
    "            'score': p['score'],\n",
    "            'type': p.get('type', '1-1')\n",
    "        })\n",
    "\n",
    "# Thống kê báo cáo\n",
    "df_res = pd.DataFrame(final_data)\n",
    "type_counts = df_res['type'].value_counts()\n",
    "\n",
    "print(\"Kết quả Alignment\")\n",
    "print(f\"Tổng số cặp: {len(df_res)}\")\n",
    "print(f\"Phân bố cặp câu: {type_counts}\")\n",
    "print(f\"Average Semantic Score: {df_res['score'].mean():.4f}\")\n",
    "\n",
    "# Lưu file\n",
    "df_res[['src_id', 'src_lang', 'tgt_lang']].to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "print(f\"Đã lưu file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb25758",
   "metadata": {},
   "source": [
    "6. Evaluation cho Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa0ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "SRC = 'PDF1'\n",
    "FROM_ID = 626\n",
    "TO_ID = 750\n",
    "INPUT_FILE = f\"../Alignment/{SRC}_{FROM_ID}_{TO_ID}.csv\" \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377696ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy_translations(df, sample_size):\n",
    "    \"\"\"\n",
    "    Dịch câu nguồn tiếng Trung sang tiếng Việt dùng Google Translate\n",
    "    để làm tham chiếu so sánh.\n",
    "    \"\"\"\n",
    "    print(f\"Đang lấy mẫu ngẫu nhiên {sample_size} cặp câu để đánh giá...\")\n",
    "    \n",
    "    # Lấy mẫu ngẫu nhiên (random_state có thể tùy ý điều chỉnh)\n",
    "    sample = df.sample(n=min(sample_size, len(df)), random_state=10).copy()\n",
    "\n",
    "    translator = Translator()\n",
    "    hypotheses = [] # Câu trong file \n",
    "    references = [] # Câu dịch máy (Làm chuẩn so sánh)\n",
    "    success_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(sample.iterrows(), total=len(sample)):\n",
    "        src = str(row['src_lang'])\n",
    "        tgt = str(row['tgt_lang']) # Đây là câu tiếng Việt gốc trong dataset\n",
    "        \n",
    "        # Bỏ qua câu quá ngắn hoặc rỗng\n",
    "        if len(src) < 2 or len(tgt) < 5: \n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Dịch Trung -> Việt\n",
    "            translated = translator.translate(src, src='zh-cn', dest='vi').text\n",
    "            \n",
    "            # Câu Google dịch là Reference (Tham chiếu) và câu Tgt gốc là Hypothesis (Giả thuyết)\n",
    "            references.append(translated) \n",
    "            hypotheses.append(tgt)\n",
    "            \n",
    "            success_count += 1\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Nếu lỗi mạng thì bỏ qua\n",
    "            continue\n",
    "            \n",
    "    print(f\"Đã dịch thành công {success_count} câu.\")\n",
    "    return hypotheses, references\n",
    "\n",
    "def calculate_metrics(hypotheses, references):\n",
    "    \"\"\"Tính toán BLEU và BERTScore\"\"\"\n",
    "    if not hypotheses:\n",
    "        print(\"Không có dữ liệu để tính toán.\")\n",
    "        return\n",
    "\n",
    "    # Tính BLEU Score\n",
    "    # BLEU đánh giá độ khớp từ vựng (n-gram overlap)\n",
    "    bleu = BLEU()\n",
    "    bleu_score = bleu.corpus_score(hypotheses, [references])\n",
    "    \n",
    "    print(f\"BLEU Score: {bleu_score.score:.2f}\")\n",
    "\n",
    "    # Tính BERTScore\n",
    "    # BERTScore đánh giá độ tương đồng ngữ nghĩa (Semantic Similarity) dùng mô hình BERT\n",
    "    P, R, F1 = score(hypotheses, references, lang='vi', verbose=False, device=DEVICE)\n",
    "    \n",
    "    avg_bert = F1.mean().item()\n",
    "    print(f\"BERTScore (F1): {avg_bert:.4f}\")\n",
    "\n",
    "    print(\"Ví dụ tham chiếu (Top 3):\")\n",
    "    for i in range(min(3, len(hypotheses))):\n",
    "        print(f\"Cặp {i+1}:\")\n",
    "        print(f\"Dataset Tgt: {hypotheses[i]}\")\n",
    "        print(f\"Google Trans: {references[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3174297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang lấy mẫu ngẫu nhiên 12 cặp câu để đánh giá...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:03<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã dịch thành công 12 câu.\n",
      "BLEU Score: 19.50\n",
      "BERTScore (F1): 0.8170\n",
      "Ví dụ tham chiếu (Top 3):\n",
      "Cặp 1:\n",
      "Dataset Tgt: Nếu bạn không khuất phục thì thế giới này có thể làm gì bạn chứ.\n",
      "Google Trans: Nếu bạn không nhượng bộ, thế giới này có thể làm gì bạn?\n",
      "Cặp 2:\n",
      "Dataset Tgt: Bức thư viết cho chính mình số 665 Cuộc sống của bạn nên vì bạn mà đa dạng sắc màu.\n",
      "Google Trans: Thư 665 gửi chính mình Cuộc sống của bạn nên có sự huy hoàng của riêng bạn.\n",
      "Cặp 3:\n",
      "Dataset Tgt: Cuộc sống có tiến có lui, thua bất cứ thứ gì thì cũng được nhưng không được mất tâm trạng phấn khích.\n",
      "Google Trans: Trong cuộc sống có những tiến bộ và rút lui, và bạn không thể đánh mất tâm trạng của mình cho dù có mất đi điều gì.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT_FILE)\n",
    "# Thực hiện quy trình đánh giá\n",
    "SAMPLE_SIZE = int(df.shape[0] * 0.1)  # Số lượng câu lấy mẫu để đánh giá \n",
    "hyps, refs = get_proxy_translations(df, SAMPLE_SIZE)\n",
    "# Tính điểm\n",
    "calculate_metrics(hyps, refs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
