{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b418f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data\\HCMUS\\Y4\\Y4_S1\\Nhap_mon_xu_ly_ngon_ngu_tu_nhien\\Mid_term\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, math, random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "INPUT_PATH = \"../data/preprocessing_data/json2.csv\" # Thay đường dẫn input_path để chạy các file khác nhau\n",
    "OUT_DIR = \"../output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_N = 5                 # 1-n alignment (1-1, 1-2, 1-3)\n",
    "BATCH_SIZE = 16           # encode batch size\n",
    "ALPHA_LEN = 0.05          # penalty độ dài \n",
    "END_TGT_PENALTY = 0.05    # phạt nhẹ phần tgt dư không dùng\n",
    "AUDIT_SEED = 0\n",
    "AUDIT_RATE = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202e77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input\n",
    "def load_input_any(path: str) -> pd.DataFrame:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    elif ext == \".json\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, dict) and \"data\" in data and isinstance(data[\"data\"], list):\n",
    "            data = data[\"data\"]\n",
    "        df = pd.DataFrame(data)\n",
    "    elif ext in [\".jsonl\", \".jl\"]:\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    rows.append(json.loads(line))\n",
    "        df = pd.DataFrame(rows)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "    df = df[[\"src_id\", \"src_lang\", \"tgt_lang\"]].copy()\n",
    "    df[\"src_id\"] = df[\"src_id\"].astype(str)\n",
    "    df[\"src_lang\"] = df[\"src_lang\"].astype(str)\n",
    "    df[\"tgt_lang\"] = df[\"tgt_lang\"].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbbb5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding (LaBSE)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "labse = SentenceTransformer(\"sentence-transformers/LaBSE\", device=DEVICE)\n",
    "\n",
    "def embed_sentences(sents: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "    if not sents:\n",
    "        return np.zeros((0, 768), dtype=np.float32)\n",
    "    emb = labse.encode(\n",
    "        sents,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=False,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    return emb.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3434621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tách câu (ZH / VI)\n",
    "def _compact_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def split_sentences_zh(text: str) -> List[str]:\n",
    "    text = _compact_spaces(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r\"(?<=[。！？!?；;])\\s*\", text)\n",
    "    return [p.strip() for p in parts if p and p.strip()]\n",
    "\n",
    "def split_sentences_vi_basic(text: str) -> List[str]:\n",
    "    text = _compact_spaces(text)\n",
    "    if not text:\n",
    "        return []\n",
    "    parts = re.split(r\"(?:(?<=[\\.\\?\\!…])\\s+)|(?:(?<=\\.\\.\\.)\\s+)\", text)\n",
    "    return [p.strip() for p in parts if p and p.strip()]\n",
    "\n",
    "def _pick_split_point(s: str, seps: List[str]) -> Optional[int]:\n",
    "    L = len(s)\n",
    "    best = None\n",
    "    for sep in seps:\n",
    "        for m in re.finditer(re.escape(sep), s):\n",
    "            cut = m.start() + len(sep)\n",
    "            if cut < 5 or cut > L - 5:\n",
    "                continue\n",
    "            score = abs(cut - L / 2)\n",
    "            if best is None or score < best[0]:\n",
    "                best = (score, cut)\n",
    "    return best[1] if best else None\n",
    "\n",
    "def refine_vi_segments(sents: List[str], min_count: int) -> List[str]:\n",
    "    sents = list(sents)\n",
    "    seps = [\";\", \":\", \",\", \" - \", \" – \", \" — \"]\n",
    "    while len(sents) < min_count:\n",
    "        best_idx, best_cut, best_len = None, None, -1\n",
    "        for i, s in enumerate(sents):\n",
    "            cut = _pick_split_point(s, seps)\n",
    "            if cut is None:\n",
    "                continue\n",
    "            if len(s) > best_len:\n",
    "                best_len = len(s)\n",
    "                best_idx = i\n",
    "                best_cut = cut\n",
    "        if best_idx is None:\n",
    "            break\n",
    "        s = sents[best_idx]\n",
    "        left = s[:best_cut].strip()\n",
    "        right = s[best_cut:].strip()\n",
    "        if not left or not right:\n",
    "            break\n",
    "        sents = sents[:best_idx] + [left, right] + sents[best_idx+1:]\n",
    "    return sents\n",
    "\n",
    "def split_sentences_vi(text: str, min_count: Optional[int] = None) -> List[str]:\n",
    "    sents = split_sentences_vi_basic(text)\n",
    "    if min_count is not None and len(sents) < min_count:\n",
    "        sents = refine_vi_segments(sents, min_count)\n",
    "    return sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b6888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment (Monotonic 1-n DP with LaBSE cosine)\n",
    "def text_len(s: str) -> int:\n",
    "    return len(re.sub(r\"\\s+\", \"\", s.strip()))\n",
    "\n",
    "def align_1_to_n_labse(\n",
    "    src_sents: List[str],\n",
    "    tgt_sents: List[str],\n",
    "    max_n: int = 3,\n",
    "    alpha_len: float = 0.15,\n",
    "    batch_size: int = 64,\n",
    "    end_tgt_penalty: float = 0.20,\n",
    "):\n",
    "    \"\"\"\n",
    "    DP monotonic: mỗi src_i -> concat tgt[j:k], 1 <= k-j <= max_n\n",
    "    Score dựa trên:\n",
    "      cost = -cosine_sim + alpha_len * length_penalty\n",
    "    Cho phép align rỗng khi tgt đã hết.\n",
    "    \"\"\"\n",
    "    m, n = len(src_sents), len(tgt_sents)\n",
    "    if m == 0:\n",
    "        return []\n",
    "    if n == 0:\n",
    "        return [(src_sents[i], \"\", 0, 0) for i in range(m)]\n",
    "\n",
    "    src_emb = embed_sentences(src_sents, batch_size=batch_size)  # normalized\n",
    "    tgt_emb = embed_sentences(tgt_sents, batch_size=batch_size)  # normalized\n",
    "\n",
    "    # prefix sum để lấy mean segment nhanh\n",
    "    d = tgt_emb.shape[1]\n",
    "    ps = np.zeros((n + 1, d), dtype=np.float32)\n",
    "    ps[1:] = np.cumsum(tgt_emb, axis=0)\n",
    "\n",
    "    def seg_mean_norm(j0: int, j1: int) -> np.ndarray:\n",
    "        v = (ps[j1] - ps[j0]) / max(1, (j1 - j0))\n",
    "        norm = np.linalg.norm(v) + 1e-9\n",
    "        return (v / norm).astype(np.float32)\n",
    "\n",
    "    # ratio kỳ vọng để penalty độ dài\n",
    "    sum_src = sum(text_len(s) for s in src_sents) or 1\n",
    "    sum_tgt = sum(text_len(t) for t in tgt_sents) or 1\n",
    "    r0 = sum_tgt / sum_src\n",
    "\n",
    "    INF = 1e18\n",
    "    dp = [[INF] * (n + 1) for _ in range(m + 1)]\n",
    "    back = [[None] * (n + 1) for _ in range(m + 1)]\n",
    "    dp[0][0] = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n + 1):\n",
    "            if dp[i][j] >= INF:\n",
    "                continue\n",
    "\n",
    "            # Nếu tgt đã hết, các src còn lại align rỗng (cost nhỏ cố định)\n",
    "            if j == n:\n",
    "                ndp = dp[i][j] + 0.50  # penalty align empty\n",
    "                if ndp < dp[i + 1][j]:\n",
    "                    dp[i + 1][j] = ndp\n",
    "                    back[i + 1][j] = (j, j)\n",
    "                continue\n",
    "\n",
    "            # thử ghép 1..max_n câu tgt\n",
    "            for k in range(j + 1, min(n, j + max_n) + 1):\n",
    "                v = seg_mean_norm(j, k)\n",
    "                sim = float(np.dot(src_emb[i], v))  # cosine because normalized\n",
    "\n",
    "                ls = text_len(src_sents[i]) or 1\n",
    "                lt = text_len(\" \".join(tgt_sents[j:k]))\n",
    "                ratio = lt / ls\n",
    "                len_pen = abs(math.log((ratio + 1e-9) / (r0 + 1e-9)))\n",
    "\n",
    "                cost = (-sim) + alpha_len * len_pen\n",
    "                ndp = dp[i][j] + cost\n",
    "\n",
    "                if ndp < dp[i + 1][k]:\n",
    "                    dp[i + 1][k] = ndp\n",
    "                    back[i + 1][k] = (j, k)\n",
    "\n",
    "    # chọn endpoint tốt nhất, phạt nhẹ tgt dư\n",
    "    best_j, best = None, INF\n",
    "    for j in range(n, -1, -1):\n",
    "        if dp[m][j] < INF:\n",
    "            score = dp[m][j] + end_tgt_penalty * (n - j)\n",
    "            if score < best:\n",
    "                best = score\n",
    "                best_j = j\n",
    "\n",
    "    if best_j is None:\n",
    "        # fallback greedy\n",
    "        aligns = []\n",
    "        j = 0\n",
    "        for i in range(m):\n",
    "            if j < n:\n",
    "                aligns.append((src_sents[i], tgt_sents[j], j, j + 1))\n",
    "                j += 1\n",
    "            else:\n",
    "                aligns.append((src_sents[i], \"\", n, n))\n",
    "        return aligns\n",
    "\n",
    "    # backtrack\n",
    "    j = best_j\n",
    "    aligns = []\n",
    "    for i in range(m, 0, -1):\n",
    "        j0, j1 = back[i][j]\n",
    "        aligns.append((src_sents[i - 1], \" \".join(tgt_sents[j0:j1]).strip(), j0, j1))\n",
    "        j = j0\n",
    "    aligns.reverse()\n",
    "    return aligns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c8bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def parse_numeric_id(x: str) -> Optional[int]:\n",
    "    m = re.search(r\"\\d+\", str(x))\n",
    "    return int(m.group(0)) if m else None\n",
    "\n",
    "def align_dataframe_labse(df_in: pd.DataFrame):\n",
    "    out_rows = []\n",
    "    n_dist = {}\n",
    "    ratios = []\n",
    "    sims = []\n",
    "\n",
    "    tgt_used_total = 0\n",
    "    tgt_total = 0\n",
    "    empty_tgt = 0\n",
    "\n",
    "    for _, row in df_in.iterrows():\n",
    "        base_id = str(row[\"src_id\"])\n",
    "        src_text = str(row[\"src_lang\"])\n",
    "        tgt_text = str(row[\"tgt_lang\"])\n",
    "\n",
    "        src_sents = split_sentences_zh(src_text)\n",
    "        tgt_sents = split_sentences_vi(tgt_text, min_count=len(src_sents))\n",
    "\n",
    "        aligns = align_1_to_n_labse(\n",
    "            src_sents, tgt_sents,\n",
    "            max_n=MAX_N,\n",
    "            alpha_len=ALPHA_LEN,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            end_tgt_penalty=END_TGT_PENALTY\n",
    "        )\n",
    "\n",
    "        used = set()\n",
    "        for i, (s, t, j0, j1) in enumerate(aligns, start=1):\n",
    "            n = j1 - j0\n",
    "            n_dist[n] = n_dist.get(n, 0) + 1\n",
    "            used.update(range(j0, j1))\n",
    "\n",
    "            ls = text_len(s)\n",
    "            lt = text_len(t)\n",
    "            if ls > 0 and lt > 0:\n",
    "                ratios.append(lt / ls)\n",
    "            if not t.strip():\n",
    "                empty_tgt += 1\n",
    "\n",
    "            out_rows.append({\n",
    "                \"src_id\": f\"{base_id}_{i}\",\n",
    "                \"src_lang\": s,\n",
    "                \"tgt_lang\": t\n",
    "            })\n",
    "\n",
    "        tgt_used_total += len(used)\n",
    "        tgt_total += len(tgt_sents)\n",
    "\n",
    "    df_out = pd.DataFrame(out_rows, columns=[\"src_id\", \"src_lang\", \"tgt_lang\"])\n",
    "\n",
    "    metrics = {\n",
    "        \"input_rows\": int(len(df_in)),\n",
    "        \"output_rows_aligned\": int(len(df_out)),\n",
    "        \"alignment_type_distribution\": {f\"1-{k}\": int(v) for k, v in sorted(n_dist.items())},\n",
    "        \"tgt_sentence_usage_coverage\": float(tgt_used_total / tgt_total) if tgt_total > 0 else None,\n",
    "        \"avg_tgt_src_len_ratio\": float(np.mean(ratios)) if ratios else None,\n",
    "        \"std_tgt_src_len_ratio\": float(np.std(ratios)) if ratios else None,\n",
    "        \"empty_tgt_pairs\": int(empty_tgt),\n",
    "        \"max_n\": int(MAX_N),\n",
    "        \"alpha_len\": float(ALPHA_LEN),\n",
    "        \"model\": \"sentence-transformers/LaBSE\"\n",
    "    }\n",
    "\n",
    "    return df_out, metrics\n",
    "\n",
    "def save_outputs(df_out: pd.DataFrame, metrics: dict, input_path: str, out_dir: str):\n",
    "    src_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    base_ids = df_out[\"src_id\"].astype(str).str.rsplit(\"_\", n=1).str[0]\n",
    "    nums = [parse_numeric_id(x) for x in base_ids.tolist()]\n",
    "    nums = [x for x in nums if x is not None]\n",
    "    from_id = min(nums) if nums else 0\n",
    "    to_id = max(nums) if nums else 0\n",
    "\n",
    "    out_csv = os.path.join(out_dir, f\"{src_name}_{from_id}_{to_id}.csv\")\n",
    "    df_out.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    out_metrics = os.path.join(out_dir, f\"{src_name}_{from_id}_{to_id}_metrics.json\")\n",
    "    with open(out_metrics, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return out_csv, out_metrics\n",
    "\n",
    "def export_audit(df_out: pd.DataFrame, input_path: str, out_dir: str, rate: float = 0.10, seed: int = 0):\n",
    "    src_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    base_ids = df_out[\"src_id\"].astype(str).str.rsplit(\"_\", n=1).str[0]\n",
    "    nums = [parse_numeric_id(x) for x in base_ids.tolist()]\n",
    "    nums = [x for x in nums if x is not None]\n",
    "    from_id = min(nums) if nums else 0\n",
    "    to_id = max(nums) if nums else 0\n",
    "    \n",
    "    n = max(1, int(rate * len(df_out)))\n",
    "    \n",
    "    sample = df_out.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "    out_audit = os.path.join(out_dir, f\"{src_name}_{from_id}_{to_id}_audit10pct.csv\")\n",
    "    sample.to_csv(out_audit, index=False, encoding=\"utf-8-sig\")\n",
    "    return out_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc020ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned CSV: ../output\\json2_4653_9255.csv\n",
      "Saved metrics JSON: ../output\\json2_4653_9255_metrics.json\n",
      "Saved audit 10%: ../output\\json2_4653_9255_audit10pct.csv\n",
      "{\n",
      "  \"input_rows\": 4603,\n",
      "  \"output_rows_aligned\": 7953,\n",
      "  \"alignment_type_distribution\": {\n",
      "    \"1-0\": 15,\n",
      "    \"1-1\": 7666,\n",
      "    \"1-2\": 246,\n",
      "    \"1-3\": 19,\n",
      "    \"1-4\": 6,\n",
      "    \"1-5\": 1\n",
      "  },\n",
      "  \"tgt_sentence_usage_coverage\": 0.9997574581615328,\n",
      "  \"avg_tgt_src_len_ratio\": 2.7560139154944947,\n",
      "  \"std_tgt_src_len_ratio\": 1.0316128716097732,\n",
      "  \"empty_tgt_pairs\": 15,\n",
      "  \"max_n\": 5,\n",
      "  \"alpha_len\": 0.05,\n",
      "  \"model\": \"sentence-transformers/LaBSE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "df_in = load_input_any(INPUT_PATH)\n",
    "df_out, metrics = align_dataframe_labse(df_in)\n",
    "\n",
    "out_csv, out_metrics = save_outputs(df_out, metrics, INPUT_PATH, OUT_DIR)\n",
    "out_audit = export_audit(df_out, INPUT_PATH, OUT_DIR, rate=AUDIT_RATE, seed=AUDIT_SEED)\n",
    "\n",
    "print(\"Saved aligned CSV:\", out_csv)\n",
    "print(\"Saved metrics JSON:\", out_metrics)\n",
    "print(\"Saved audit 10%:\", out_audit)\n",
    "print(json.dumps(metrics, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88acd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_id</th>\n",
       "      <th>src_lang</th>\n",
       "      <th>tgt_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4653_1_1</td>\n",
       "      <td>我认为这将是主要的事情。</td>\n",
       "      <td>Tôi nghĩ rằng đó sẽ là điều chính.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4653_1_2</td>\n",
       "      <td>我认为，同意某人所说，应该选择HSV-1血清阳性且携带APOE4等位基因的人。</td>\n",
       "      <td>Tôi nghĩ, đồng ý với người nào đó nói rằng nên...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4654_1_1</td>\n",
       "      <td>我非常想说这个。</td>\n",
       "      <td>Tôi rất muốn nói điều này.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4654_1_2</td>\n",
       "      <td>我相信这是从许多问题中得出的结论。</td>\n",
       "      <td>Tôi chắc chắn điều này được tìm kiếm từ rất nh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4654_1_3</td>\n",
       "      <td>史蒂文·雅各布森：但是马克，我可以提个建议吗？</td>\n",
       "      <td>Steven Jacobson: Nhưng Mack, tôi có thể đưa ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     src_id                                 src_lang  \\\n",
       "0  4653_1_1                             我认为这将是主要的事情。   \n",
       "1  4653_1_2  我认为，同意某人所说，应该选择HSV-1血清阳性且携带APOE4等位基因的人。   \n",
       "2  4654_1_1                                 我非常想说这个。   \n",
       "3  4654_1_2                        我相信这是从许多问题中得出的结论。   \n",
       "4  4654_1_3                  史蒂文·雅各布森：但是马克，我可以提个建议吗？   \n",
       "\n",
       "                                            tgt_lang  \n",
       "0                 Tôi nghĩ rằng đó sẽ là điều chính.  \n",
       "1  Tôi nghĩ, đồng ý với người nào đó nói rằng nên...  \n",
       "2                         Tôi rất muốn nói điều này.  \n",
       "3  Tôi chắc chắn điều này được tìm kiếm từ rất nh...  \n",
       "4  Steven Jacobson: Nhưng Mack, tôi có thể đưa ra...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
