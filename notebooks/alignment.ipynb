{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be7945f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f667f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = 'PDF1'\n",
    "FROM_ID = 501\n",
    "TO_ID = 625\n",
    "# INPUT_FILE = f\"../data/preprocessing_data/{SRC.lower()}_{FROM_ID}_{TO_ID}.csv\"\n",
    "INPUT_FILE = f\"../output/ocr/{SRC}_{FROM_ID}_{TO_ID}.csv\"\n",
    "OUTPUT_FILE = f\"../output/alignment/{SRC}_{FROM_ID}_{TO_ID}.csv\"\n",
    "ALIGN_THRESHOLD = 0.75\n",
    "MIN_SENTENCE_LENGTH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a14f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    return 'cpu'\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'^[^:\\n]+:\\s*', '', text) # Bỏ tên người nói\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def split_sentences(text, lang='vi'):\n",
    "    text = clean_text(text)\n",
    "    if not text: return []\n",
    "    if lang == 'zh': \n",
    "        sents = re.split(r'(?<=[。！？])\\s*', text)\n",
    "    else: \n",
    "        sents = re.split(r'(?<=[.?!])\\s+(?=[A-Z\"\\'(])', text)\n",
    "    return [s.strip() for s in sents if len(s.strip()) >= MIN_SENTENCE_LENGTH]\n",
    "\n",
    "def align_n3(src_sents, tgt_sents, model, device, threshold):\n",
    "    if not src_sents or not tgt_sents:\n",
    "        return []\n",
    "\n",
    "    # Encode\n",
    "    # 1-gram (Câu đơn)\n",
    "    emb_src = model.encode(src_sents, convert_to_tensor=True, device=device)\n",
    "    emb_tgt = model.encode(tgt_sents, convert_to_tensor=True, device=device)\n",
    "    \n",
    "    # 2-gram (Gộp 2 câu)\n",
    "    tgt_merged_2 = [tgt_sents[i] + \" \" + tgt_sents[i+1] for i in range(len(tgt_sents)-1)]\n",
    "    src_merged_2 = [src_sents[i] + src_sents[i+1] for i in range(len(src_sents)-1)]\n",
    "    \n",
    "    emb_tgt_2 = model.encode(tgt_merged_2, convert_to_tensor=True, device=device) if tgt_merged_2 else None\n",
    "    emb_src_2 = model.encode(src_merged_2, convert_to_tensor=True, device=device) if src_merged_2 else None\n",
    "\n",
    "    # 3-gram (Gộp 3 câu) \n",
    "    tgt_merged_3 = [tgt_sents[i] + \" \" + tgt_sents[i+1] + \" \" + tgt_sents[i+2] for i in range(len(tgt_sents)-2)]\n",
    "    src_merged_3 = [src_sents[i] + src_sents[i+1] + src_sents[i+2] for i in range(len(src_sents)-2)]\n",
    "    \n",
    "    emb_tgt_3 = model.encode(tgt_merged_3, convert_to_tensor=True, device=device) if tgt_merged_3 else None\n",
    "    emb_src_3 = model.encode(src_merged_3, convert_to_tensor=True, device=device) if src_merged_3 else None\n",
    "\n",
    "    # Tính ma trận tương đồng (similarity matrices)\n",
    "    # Khởi tạo ma trận (rỗng chiều)\n",
    "    def get_sim(emb1, emb2):\n",
    "        if emb1 is not None and emb2 is not None:\n",
    "            return util.cos_sim(emb1, emb2).cpu().numpy()\n",
    "        return np.zeros((0,0))\n",
    "\n",
    "    sim_1_1 = get_sim(emb_src, emb_tgt)\n",
    "    \n",
    "    sim_1_2 = get_sim(emb_src, emb_tgt_2)     # Src đơn vs Tgt gộp 2\n",
    "    sim_2_1 = get_sim(emb_src_2, emb_tgt)     # Src gộp 2 vs Tgt đơn\n",
    "    \n",
    "    sim_1_3 = get_sim(emb_src, emb_tgt_3)     # Src đơn vs Tgt gộp 3 \n",
    "    sim_3_1 = get_sim(emb_src_3, emb_tgt)     # Src gộp 3 vs Tgt đơn\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Đánh dấu câu đã dùng\n",
    "    used_src = np.zeros(len(src_sents), dtype=bool)\n",
    "    used_tgt = np.zeros(len(tgt_sents), dtype=bool)\n",
    "\n",
    "    # Greedy search\n",
    "    while True:\n",
    "        best_score = -1\n",
    "        best_type = None\n",
    "        best_indices = None # (r, c)\n",
    "\n",
    "        # Helper để check max score trong matrix\n",
    "        def check_matrix(matrix, label):\n",
    "            nonlocal best_score, best_type, best_indices\n",
    "            if matrix.size > 0:\n",
    "                r, c = np.unravel_index(matrix.argmax(), matrix.shape)\n",
    "                if matrix[r, c] > best_score:\n",
    "                    best_score = matrix[r, c]\n",
    "                    best_type = label\n",
    "                    best_indices = (r, c)\n",
    "\n",
    "        # Kiểm tra tất cả các trường hợp\n",
    "        check_matrix(sim_1_1, '1-1')\n",
    "        check_matrix(sim_1_2, '1-2')\n",
    "        check_matrix(sim_2_1, '2-1')\n",
    "        check_matrix(sim_1_3, '1-3') \n",
    "        check_matrix(sim_3_1, '3-1') \n",
    "\n",
    "        if best_score < threshold:\n",
    "            break\n",
    "\n",
    "        r, c = best_indices\n",
    "        valid_match = False\n",
    "        \n",
    "        # Logic kết quả và masking\n",
    "        # Logic chung: Kiểm tra used -> Add result -> Mark used -> Mask matrix\n",
    "        \n",
    "        if best_type == '1-1':\n",
    "            if not used_src[r] and not used_tgt[c]:\n",
    "                results.append({'src': src_sents[r], 'tgt': tgt_sents[c], 'type': '1-1'})\n",
    "                used_src[r] = True; used_tgt[c] = True\n",
    "                valid_match = True\n",
    "                # Masking: Xóa hàng r (src) và cột c (tgt) ở mọi matrix liên quan: gán -1 trực tiếp vào các vùng ảnh hưởng\n",
    "                sim_1_1[r, :] = -1; sim_1_1[:, c] = -1\n",
    "                if sim_1_2.size: sim_1_2[r, :] = -1; sim_1_2[:, max(0, c-1):c+1] = -1\n",
    "                if sim_2_1.size: sim_2_1[max(0, r-1):r+1, :] = -1; sim_2_1[:, c] = -1\n",
    "                if sim_1_3.size: sim_1_3[r, :] = -1; sim_1_3[:, max(0, c-2):c+1] = -1\n",
    "                if sim_3_1.size: sim_3_1[max(0, r-2):r+1, :] = -1; sim_3_1[:, c] = -1\n",
    "\n",
    "        elif best_type == '1-2': # Src r vs Tgt c, c+1\n",
    "            if not used_src[r] and not used_tgt[c] and not used_tgt[c+1]:\n",
    "                results.append({'src': src_sents[r], 'tgt': tgt_sents[c] + \" \" + tgt_sents[c+1], 'type': '1-2'})\n",
    "                used_src[r] = True; used_tgt[c:c+2] = True\n",
    "                valid_match = True\n",
    "                sim_1_2[r, c] = -1 # Xóa chính nó\n",
    "                sim_1_1[r, :] = -1; sim_1_1[:, c:c+2] = -1\n",
    "               \n",
    "\n",
    "        elif best_type == '2-1': # Src r, r+1 vs Tgt c\n",
    "            if not used_src[r] and not used_src[r+1] and not used_tgt[c]:\n",
    "                results.append({'src': src_sents[r]+src_sents[r+1], 'tgt': tgt_sents[c], 'type': '2-1'})\n",
    "                used_src[r:r+2] = True; used_tgt[c] = True\n",
    "                valid_match = True\n",
    "                sim_2_1[r, c] = -1\n",
    "                sim_1_1[r:r+2, :] = -1; sim_1_1[:, c] = -1\n",
    "\n",
    "        elif best_type == '1-3': # Src r vs Tgt c, c+1, c+2\n",
    "            if not used_src[r] and not used_tgt[c] and not used_tgt[c+1] and not used_tgt[c+2]:\n",
    "                results.append({'src': src_sents[r], 'tgt': tgt_sents[c]+\" \"+tgt_sents[c+1]+\" \"+tgt_sents[c+2], 'type': '1-3'})\n",
    "                used_src[r] = True; used_tgt[c:c+3] = True\n",
    "                valid_match = True\n",
    "                sim_1_3[r, c] = -1\n",
    "                sim_1_1[r, :] = -1; sim_1_1[:, c:c+3] = -1\n",
    "\n",
    "        elif best_type == '3-1': # Src r, r+1, r+2 vs Tgt c\n",
    "            if not used_src[r] and not used_src[r+1] and not used_src[r+2] and not used_tgt[c]:\n",
    "                results.append({'src': src_sents[r]+src_sents[r+1]+src_sents[r+2], 'tgt': tgt_sents[c], 'type': '3-1'})\n",
    "                used_src[r:r+3] = True; used_tgt[c] = True\n",
    "                valid_match = True\n",
    "                sim_3_1[r, c] = -1\n",
    "                sim_1_1[r:r+3, :] = -1; sim_1_1[:, c] = -1\n",
    "\n",
    "        # Nếu không match được (do conflict đã bị dùng) -> xóa điểm này để tìm max tiếp theo\n",
    "        if not valid_match:\n",
    "            if best_type == '1-1': sim_1_1[r, c] = -1\n",
    "            elif best_type == '1-2': sim_1_2[r, c] = -1\n",
    "            elif best_type == '2-1': sim_2_1[r, c] = -1\n",
    "            elif best_type == '1-3': sim_1_3[r, c] = -1\n",
    "            elif best_type == '3-1': sim_3_1[r, c] = -1\n",
    "\n",
    "        # Cập nhật kết quả\n",
    "        if valid_match:\n",
    "            results[-1]['score'] = float(best_score)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376ca2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải model LaBSE trên cuda (n=3)...\n",
      "Bắt đầu Alignment (1-1, 1-2, 2-1, 1-3, 3-1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:04<00:00, 27.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kết quả Alignment\n",
      "Tổng số cặp: 125\n",
      "Phân bố cặp câu: type\n",
      "1-1    86\n",
      "1-2    23\n",
      "2-1     8\n",
      "1-3     7\n",
      "3-1     1\n",
      "Name: count, dtype: int64\n",
      "Average Semantic Score: 0.8053\n",
      "Đã lưu file: ../output/alignment/PDF1_501_625.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(f\"Đang tải model LaBSE trên {device} (n=3)...\")\n",
    "model = SentenceTransformer('sentence-transformers/LaBSE', device=device)\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "final_data = []\n",
    "\n",
    "print(\"Bắt đầu Alignment (1-1, 1-2, 2-1, 1-3, 3-1)...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    src_sents = split_sentences(row.get('src_lang', ''), lang='zh')\n",
    "    tgt_sents = split_sentences(row.get('tgt_lang', ''), lang='vi')\n",
    "    \n",
    "    pairs = align_n3(src_sents, tgt_sents, model, device, ALIGN_THRESHOLD)\n",
    "    \n",
    "    for p in pairs:\n",
    "        final_data.append({\n",
    "            'src_id': row.get('src_id', ''),\n",
    "            'src_lang': p['src'],\n",
    "            'tgt_lang': p['tgt'],\n",
    "            'score': p['score'],\n",
    "            'type': p.get('type', '1-1')\n",
    "        })\n",
    "\n",
    "# Thống kê báo cáo\n",
    "df_res = pd.DataFrame(final_data)\n",
    "type_counts = df_res['type'].value_counts()\n",
    "\n",
    "print(\"Kết quả Alignment\")\n",
    "print(f\"Tổng số cặp: {len(df_res)}\")\n",
    "print(f\"Phân bố cặp câu: {type_counts}\")\n",
    "print(f\"Average Semantic Score: {df_res['score'].mean():.4f}\")\n",
    "\n",
    "# Lưu file\n",
    "df_res[['src_id', 'src_lang', 'tgt_lang']].to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "print(f\"Đã lưu file: {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
